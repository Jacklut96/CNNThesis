\documentclass[../main.tex]{subfiles}
\begin{document}
%\chapter{Validation, evaluation,  potential}
\label{capitolo4}
\thispagestyle{empty}

In this chapter the results obtained from the already described tests \ref{capitolo3} will be observed. Such results will then be analysed and discussed, contestualising them into the project concept and considering their value.

\section{Test Setup}
All the tests have been executed on a machine equipped with an Intel \textbf{i7 3,40GHz} processor, \textbf{16GB} of RAM, running \textbf{CentOS7}: a Linux distribution; no GPU has been put into use.
Due to time constraints tests are performed only on two differently labeled Datasets, whose preparation has already been fully disclosed\ref{capitolo3}: \textit{Lungs/NotLungs} and \textit{Skull/NotSkull}. Even though the following results are relative only to those sections, this does not affect at all the value of the architecture itslef. As it will later be shown, the obtained results are satisfactory enough to almost assure that the model is able of detecting any kind of anatomic site with very high accuracy results.\\
\vspace{5mm}
\subsection{Execution Time}
With a source code entirely written by means of \textit{Python} and datasets with lots of images, long computational times are to be expected. On the aforementioned architecture \textit{"training/testing"} times space within a range of around 30 minutes to 10 hours, being extremely dependant on used MiniDataset dimension and machine's performance. Such a highly time-costing process, though, is one-time only, because once trained the ready model is stored and can then be loaded and extensively used without any kinds of limitation. When the already trained model is used to perform classification on never seen images, a process which will be referred to as \textit{"predicting"}, execution time is estimated at no more than 3 minutes for a 3000 images dataset. Considering how full CT exams rarely have more than 500 images, it is possible to conclude that the expected time for Prediction is even lower, even though most of its computational cost comes from loading the trained model.

\section{Results}
The results obtained will be presented in two different tables and their relative images: one for the \textit{Lungs/NotLungs} dataset and the other for the \textit{Skull/NotSkull} one.  \\
Every table will though have the same attributes, whose description is preventively given:

\begin{itemize}
	\item \textit{Train Accuracy:} Percentage of \textit{"correctly guessed"} (label assigned by the CNN and ground truth match) images in the last epoch (10th) of training.
	\item \textit{Train Loss:} Value of the loss function in the last epoch (10th) of training.
	\item \textit{Validation Accuracy:} Percentage of correctly guessed images in the last validation step (full trained network).
	\item \textit{Validation Loss:} Value of the loss function in the last validation step (full trained network).
	\item \textit{Prediction Accuracy:} Percentage of correctly guessed images in the prediction dataset.
\end{itemize}

The difference between Validation Accuracy and Prediction Accuracy is of utmost importance: Validation images are seen by the CNN at the end of every step of training, working as a "training field" on which the Net can check its performance and further learn. It is a fundamental step in machine learning, but its results are more than often not valuable as benchmarks. Thus the need of the \textit{"Prediction Set"}: all of the images it contains are never seen by the Network during its training, so the results provided by this step are functional to be defined as benchmarks for the CNN's efficiency.  \\
Furthermore, as already stated\ref{tab:testsperdata}, for some dataset sizes more than one test has been executed: in such cases the shown value will be the exact \textbf{mean} of all the achieved results.

\subsection{Lungs/NotLungs}
The following table shows the results coming from the test performed on the Lungs/NotLungs Dataset. The whole dataset is composed of 25.000 images labeled as \textit{"Lungs"} and 75.000 labeled as \textit{"NotLungs"}. As already stated, such a difference in size between the "target" and the "general" sections is to be expected as Lungs, in the considered scenario, compose only a part of the human body. The process which describes how the \textit{Prediction Accuracy}, which can be considered the "final" result, is achieved has already been completely explained.\ref{capitolo3}

\begin{table}
\caption{\textit{Lungs/Not Lungs} Results}
\label{tab:LNLres}

\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Dataset Size & Train Acc. (\%) & Train Loss & Validation Acc. (\%) & Validation Loss & \textbf{Prediction Acc. (\%)} \\
\hline
3.000         & 98,92          & 0,0325     & 93,06               & 0,2410          & \textbf{96,91}               \\
4.000         & 99,69          & 0,0090     & 96,09               & 0,1756          & \textbf{97,13}               \\
5.000         & 99,69          & 0,0105     & 97,36               & 0,1316          & \textbf{97,43}               \\
8.000         & 99,72          & 0,0087     & 97,19               & 0,1356          & \textbf{97,60}               \\
10.000        & 99,56          & 0,0131     & 97,89               & 0,0962          & \textbf{97,89}               \\
20.000        & 99,55          & 0,0117     & 97,96               & 0,0614          & \textbf{97,7}                \\
50.000        & 99,59          & 0,0115     & 98,65               & 0,0447          & \textbf{98.6}                \\
\hline
\end{tabular}}
\end{table}

As it can be seen in the table above \textbf{Training, Validation and Preditcion Accuracy}, almost all proportionally rise with the \textbf{Dataset Size}. \textit{Training Accuracy} is always the highest obtained value, as it has to be expected: The images in the \textit{"Training Set"} are the ones used to learn, if their related accuracy drops low or is unable to rise above a certain treshold \textit{(problematic tresholds are at around 60-80\% on a trial and error basis)} it is a possible sign of \textit{overfitting} which has to be absolutely removed. Another really valuable and noticeable information is the similarity between \textit{Validation Accuracy} and \textit{Prediction Accuracy} in almost every MiniDataset: Usually Validation Accuracy tends to be an overextimation of the CNN's potential as already staded, which is why the \textit{Prediction Accuracy} should be used as a benchmark.\ref{fig:lungacc}\\
A similar but specular consideration can be done with \textbf{Train and Validation Loss}, which both proportionally decrease as more images are added to the MiniDataset. Even though loss function\ref{sec:loss} should be as low as possible there is no golden standard to compare it to, thus making it an almost sterile parameter for the considered work. The only possible control that can be done on these values is a comparison between \textit{Train Loss} and \textit{Accuracy Loss}, where the first has to always be lower than the latter: if this condition is not satisfied there might be some issues with training images.\ref{fig:lungloss}\\
Even though \textit{Prediction Accuracy}, the defined benchmark for the architecture, is always really high, never falling below 96\%, there is a considerable gap between the 3.000 images dataset and the 50.000 one. Training the 3.000 dataset on the used machine takes less than 1 hour, whilst training the 50.000 one takes around 10 hours. The difference in time consumption between these MiniDatasets is quite huge, but taking into consideration the fact that is a one-time only cost to sustain and that in the medical field a 2\% difference is extremely important, it is possible to assure that the 50.000 images dataset is the \textbf{best} possible choice between all the considered ones. This also finds confirmation due to the fact that no difference in execution time comes out whilst using the already trained model to only execute predictions.

\begin{figure}[h!b]
	\centering
	\includegraphics[width=\linewidth]{lungacc.png}
	\caption{\textit{Lungs/NotLungs} Accuracy values}
	\label{fig:lungacc}
\end{figure}


\begin{figure}[h!b]
	\centering
	\includegraphics[width=\linewidth]{lungloss.png}
	\caption{\textit{Lungs/NotLungs} Loss Function values}
	\label{fig:lungloss}
\end{figure}

\subsection{Skull/NotSkull}
The results for this tests come out with the same structure as the ones before. The complete dataset is composed of 11.000 images labeled as \textit{"Skull"} and 89.000 images labeled as \textit{"NotSkull"}. The difference between these two is even bigger than the one observed in the \textit{Lungs/NotLungs} set, due to the disparity in size between skull and lungs, the latter being clearly more dimensionally relevant. The methods used to obtain these are the same as before.\ref{capitolo3}

\begin{table}
\caption{\textit{Skull/Not Skull} Results}
\label{tab:SNSres}
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Dataset Size & Train Acc. (\%) & Train Loss & Validation Acc. (\%) & Validation Loss & \textbf{Prediction Acc. (\%)} \\
\hline
3.000         & 1                   & 2,42E-04   & 99,65                    & 0,0129          & \textbf{99,53}                    \\
4.000         & 99,97               & 0,0059     & 99,87                    & 0,0021          & \textbf{99,69}                    \\
5.000         & 1                   & 8,28E-05   & 99,79                    & 0,0137          & \textbf{99,65}                    \\
8.000         & 1                   & 3,27E-05   & 99,88                    & 0,0068          & \textbf{99,69}                    \\
10.000        & 1                   & 8,00E-05   & 99,70                    & 0,0022          & \textbf{99,69}                    \\
20.000        & 1                   & 0,0016     & 99,67                    & 0,0111          & \textbf{99,90}                    \\
\hline
\end{tabular}}
\end{table}

Exactly as expected, \textit{Skull/NotSkull} follows the same logic as \textit{Lungs/NotLungs}, with \textbf{Train, Validation and Prediction Accuracy} proportionally rising with the number of images in the MiniDataset and \textbf{Train and Validation Loss} decreasing with it. The overall higher accuracy of this test compared with the other one is to be expected and can be easily explained: skulls, in CT scans, are extremely different from other body parts. On the other side lungs, being in the thoracic cage, have a bigger risk of being misinterpreted as other anatomic sites.\ref{fig:ctcomparison} \\
Even though for most part the results are exceptional and coherent, there are some particular situations which are present in this tests which have to be addressed: First of all the \textit{4000 images MiniDataset}, which not only has the lowest \textit{Training Accuracy} but, more importantly, shows a \textit{Validation Loss} lower than the relative \textit{Train Loss}: due to this only happening in such MiniDataset and not presenting anymore it is possible to appoint it to some error during the setup which does not affect the test as a whole and can be almost completely ignored.\ref{fig:skullloss} It also has to be noticed how the expected \textit{Prediction Accuracy} increase was basically not present in the 4.000 to 10.000 datasets, whilst appearing with a really high (proportionally) augmentation in the 20.000 images dataset.\ref{fig:skullacc} This confirms what was also stated before, so that the one-time huge cost to train a CNN with as much data as possible is repaid by a better accuracy. Even in this different section, indeed, we could confirm the biggest dataset (20.000 images) being the \textbf{best} one. 

\begin{figure}[h!b]
	\centering
	\includegraphics[width=\linewidth]{ctcomparison.png}
	\caption{(A)Skull Section (B)Lungs Section (C)Abdomen Section}
	\label{fig:ctcomparison}
\end{figure}

\begin{figure}[h!b]
	\centering
	\includegraphics[width=\linewidth]{skullacc.png}
	\caption{\textit{Skull/NotSkull} Accuracy values}
	\label{fig:skullacc}
\end{figure}


\begin{figure}[h!b]
	\centering
	\includegraphics[width=\linewidth]{skullloss.png}
	\caption{\textit{Skull/NotSkull} Loss Function values}
	\label{fig:skullloss}
\end{figure}

\section{Errors and Solution}
Even though the best \textbf{Prediction Accuracies} come out as \textbf{98.6\%} for \textit{Lungs/NotLungs} and an astonishing \textbf{99.9\%} for \textit{Skull/NotSkull}; they're still below the perfect \textit{100\%} mark which would ideally be the target. \\
A \textit{Python} script is used in order to identify which images are not being correctly identified and to understand the nature of the error. The possibility of the model \textbf{overfitting} (the model is adapting too well to training data and failing to predict further never-seen data\cite{overfitting}) is always present in machine learning and must be avoided at any cost. 
Focusing on the \textit{"Lungs/NotLungs"} Dataset, which is the one showing more errors, it is possible to identify some images which are labeled as \textit{Lungs} but predicted as \textit{NotLungs}.\ref{fig:lungerror, fig:lungerror2} This is the worst possible error, as it causes for important images belonging to the target section to be completely ignored. What comes out from such an analysis is that all the wrongly predicted images are not \textit{real} errors of the Net rather than labeling mistakes. Most of the time the shoulder blades and trachea sections, which have minimum if no trace at all of lungs in them, are labeled as belonging to the target section whilst they do not.\ref{fig:lungblade} The same might happen with the lower part of lungs, the one confining with the stomach. Thanks to such tests it is possible to assume that the errors are not within the used CNN's but rather during the \textbf{labeling} step. This is to be attributed to inexperience in medical image labeling: labes taken too wide \textit{(i.e. shoulder blades considered as lungs)} or too short \textit{(i.e. lungs' sections closer to the abdomen labeled as not lungs)} end up causing the aforenoticed errors. With images correctly labeled by professional doctors such a problem should not be present at all and it should then be possibile to try and achieve the desired perfect performance.

\begin{figure}[h!b]
	\centering
	\includegraphics[width=0.6\linewidth]{lungerror.png}
	\caption{Lungs/NotLungs classification error (1)}
	\label{fig:lungerror}
\end{figure}

\begin{figure}[h!b]
	\centering
	\includegraphics[width=0.6\linewidth]{lerror.png}
	\caption{Lungs/NotLungs classification error (2)}
	\label{fig:lungerror2}
\end{figure}

\begin{figure}[h!b]
	\centering
	\includegraphics[width=0.6\linewidth]{lblade.png}
	\caption{Near shoulder blades lungs section}
	\label{fig:lungblade}
\end{figure}

\section{Conclusions}

\subsection{Comparisons}
A direct comparison with some recent systems used for such task is not possible, since no standard medical dataset is available that can be used to benchmark the retrieval system,\cite{Qayyum2017} and the few other papers concerning automatic body-part recognition focus on multi-class classification tasks, whilst the here presented project only focuses on two classes at time representing the target section and its counterpart.\\
Considering all the papers describing other deep-learning methods there is one crucial trait to be addressed: A prediction dataset to execute results validation it is not used (or even mentioned at all)\cite{Qayyum2017}, or rather only just very few images \textit{(two)} are used to draw conclusions\cite{Lakhani2018}. This is a very poor evaluation method because, as it has already been stated, fails to detect overfitting and also overvalues the prediction capability of the trained network. Examples of what just stated are \textit{Roth et al.}, which work on 4928 images (splitted in 80\% train - 20\% test) belonging to five different classes, achieving an accuracy of \textbf{94.1\%} by using a LeNet inspired architecture.\cite{Roth2015} Even though the proposed results happen to be optimal there is no mention of an additional third dataset other than training and test ones, so it is unclear whether such results will be similar on different inputs rather than the training ones. Also the already cited work by \textit{Qayyum et al.}, which uses a LeNet architecture as well, achieves an average 99.77\% accuracy on 7200 images (70\% train - 30\% test) belonging to 24 classes. Yet again, though, there is no mention of a third dataset to evaluate the obtained results on images never seen by the network.\cite{Qayyum2017}\\
Whilst instead considering \textit{"Traditional Methods"} some parameters have to be defined, being the ones to be used in feature extraction and classification. In their work \textit{Aboud et al.}\cite{Aboud2015} use different parameters to identify five different classes, firstly training different combination of parameters on 500 images and then evaluating on 250 images only the most promising ones \textit{(train accuracy at least 90\%).} The used feature extraction methods are:
\begin{itemize}
	\item Color Extraction
	\item Texture Extraction
	\item Histogram of Oriented Gradents \textit{(HOG)}
\end{itemize}
As already said, the most promising training set are then classified with usage of \textbf{k-Nearest Neighbour} method: The best obtained result on a testing set with the aforemetioned process comes at 73\%, way lower than any deep-learning method.
\vspace{5mm}
\subsection{Conlcusions and Future Works}
Training via transfer learning an high-performing CNN such as the used \textbf{GoogleNet InceptionV3} proves to be extremely effective on a binary body-part classification task (with less images needed when the two classes are very different from each other \ref{tab:SNSres}), being completely able to compare with other \textit{State of the Art} classifiers. A possible future work could include merging different binary trained models for each desired section \textit{(i.e.: skull/notskull + lungs/notlungs + liver/notliver + legs/notlegs, etc.)}. The resulting multi-structured architecture might prove to outperform the current State of Art, consisting in multi-classifing LeNets.

\end{document}