\documentclass[../main.tex]{subfiles}
\begin{document}
%\chapter{Validation, evaluation,  potential}
\label{capitolo4}
\thispagestyle{empty}

In this chapter the results obtained from the methods described (Chapter \ref{capitolo3}) will be observed. Such results will then be analysed and discussed, contextualising them into the project concept and considering their value.

\section{Test Setup}
All the tests have been executed on a machine equipped with an Intel \textbf{i7 3,40GHz} processor, \textbf{16GB} of RAM, running \textbf{CentOS7}: a Linux distribution; no GPU has been put into use.
Due to time constraints tests are performed only on two differently labeled Datasets, whose preparation has already been fully disclosed (see Chapter \ref{capitolo3}): \textit{Lungs/NotLungs} and \textit{Skull/NotSkull}. Even though the following results are relative only to those sections, this does not affect at all the value of the architecture itself. As it will later be shown, the obtained results are satisfactory enough to almost assure that the model is able of detecting any kind of anatomic site with very high accuracy results.\\
\vspace{5mm}
\subsection{Execution Time}
With a source code entirely written by means of \textit{Python} and datasets with lots of images, long computational times are to be expected. On the aforementioned architecture \textit{"training/testing"} times space within a range of around 30 minutes to 10 hours, being extremely dependant on used MiniDataset dimension and machine's performance. Such a highly time-costing process, though, is one-time only, because once trained the ready model is stored and can then be loaded and extensively used without any kinds of limitation. When the already trained model is used to perform classification on never seen images, a process which will be referred to as \textit{"predicting"}, execution time is estimated at no more than 3 minutes for a 3000 images dataset. Considering how full CT exams rarely have more than 500 images, it is possible to conclude that the expected time for Prediction is even lower, even though most of its computational cost comes from loading the trained model.

\section{Results}
The results obtained will be presented in two different tables and their relative images: one for the \textit{Lungs/NotLungs} dataset and the other for the \textit{Skull/NotSkull} one.  \\
Every table will though have the same attributes, whose description is preventively given:

\begin{itemize}
	\item \textit{Train Accuracy:} Percentage of \textit{"correctly guessed"} (label assigned by the CNN and ground truth match) images in the last epoch (10th) of training.
	\item \textit{Train Loss:} Value of the loss function in the last epoch (10th) of training.
	\item \textit{Validation Accuracy:} Percentage of correctly guessed images in the last validation step (full trained network).
	\item \textit{Validation Loss:} Value of the loss function in the last validation step (full trained network).
	\item \textit{Prediction Accuracy:} Percentage of correctly guessed images in the prediction dataset.
\end{itemize}

The difference between Validation Accuracy and Prediction Accuracy is of utmost importance: Validation images are seen by the CNN at the end of every step of training, working as a "training field" on which the Net can check its performance and further learn. It is a fundamental step in machine learning, but its results are more than often not valuable as benchmarks. Thus the need of the \textit{"Prediction Set"}: all of the images it contains are never seen by the Network during its training, so the results provided by this step are functional to be defined as benchmarks for the CNN's efficiency.  \\
Furthermore, as already stated, for some dataset sizes more than one test has been executed: in such cases the shown value will be the exact \textbf{mean} of all the achieved results.
\clearpage
\newpage 

\begin{table}
\caption{\textit{Lungs/Not Lungs} Results}
\label{tab:LNLres}

\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Dataset Size & Train Acc. (\%) & Train Loss & Validation Acc. (\%) & Validation Loss & \textbf{Prediction Acc. (\%)} \\
\hline
3.000         & 98,92          & 0,0325     & 93,06               & 0,2410          & \textbf{96,91}               \\
4.000         & 99,69          & 0,0090     & 96,09               & 0,1756          & \textbf{97,13}               \\
5.000         & 99,69          & 0,0105     & 97,36               & 0,1316          & \textbf{97,43}               \\
8.000         & 99,72          & 0,0087     & 97,19               & 0,1356          & \textbf{97,60}               \\
10.000        & 99,56          & 0,0131     & 97,89               & 0,0962          & \textbf{97,89}               \\
20.000        & 99,55          & 0,0117     & 97,96               & 0,0614          & \textbf{97,7}                \\
50.000        & 99,59          & 0,0115     & 98,65               & 0,0447          & \textbf{98.6}                \\
\hline
\end{tabular}}
\end{table}


\subsection{Lungs/NotLungs}
The following table shows the results coming from the test performed on the Lungs/NotLungs Dataset. The whole dataset is composed of 25.000 images labeled as \textit{"Lungs"} and 75.000 labeled as \textit{"NotLungs"}. As already stated, such a difference in size between the "target" and the "general" sections is to be expected as Lungs, in the considered scenario, compose only a part of the human body. The process which describes how the \textit{Prediction Accuracy}, which can be considered the "final" result, is achieved has already been completely explained.(see Chapter \ref{capitolo3})

As it can be seen in Table \ref{tab:LNLres}, \textbf{Training, Validation and Prediction Accuracy} rise proportionally with the \textbf{Dataset Size}. \textit{Training Accuracy} is always the highest obtained value, as it has to be expected: the images in the \textit{"Training Set"} are the ones used to learn, if their related accuracy drops low or is unable to rise above a certain threshold \textit{(problematic thresholds are at around 60-80\% on a trial and error basis)} it is a possible sign of \textit{overfitting} which has to be absolutely removed. Another really valuable and noticeable information is the similarity between \textit{Validation Accuracy} and \textit{Prediction Accuracy} in almost every Dataset used: usually Validation Accuracy tends to be an overestimation of the CNN's potential as already stated, which is why the \textit{Prediction Accuracy} should be used as a benchmark.\\
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{lungacc.png}
	\caption{\textit{Lungs/NotLungs} Accuracy values}
	\label{fig:lungacc}
\end{figure}

Plotting the table result on a graph, further highlights the Training Accuracy to be the highest value and the similarity between Validation and Prediction Accuracy \textit{(Figure \ref{fig:lungacc})} \\
A similar but specular consideration can be done with \textbf{Train and Validation Loss}, which both proportionally decrease as more images are added to the Dataset used. Even though loss function should be as low as possible there is no golden standard to compare it to, thus making it an almost sterile parameter for the considered work. The only possible control that can be done on these values is a comparison between \textit{Train Loss} and \textit{Accuracy Loss}, where the first has to always be lower than the latter: if this condition is not satisfied there might be some issues with training images.\\
\vspace{5mm}
Even though \textit{Prediction Accuracy}, the defined benchmark for the architecture, is always really high, never falling below 96\%, there is a considerable gap between the 3.000 images dataset and the 50.000 one regarding the loss value. \textit{(Figure \ref{fig:lungloss})} \\
Training the 3.000 dataset on the used machine takes less than 1 hour, whilst training the 50.000 one takes around 10 hours. The difference in time consumption between these MiniDatasets is quite huge, but taking into consideration the fact that is a one-time only cost to sustain and that in the medical field a 2\% difference is extremely important, it is possible to assure that the 50.000 images dataset is the \textbf{best} possible choice between all the considered ones. This also finds confirmation due to the fact that no difference in execution time comes out whilst using the already trained model to only execute predictions.

\begin{figure}[h!b]
	\centering
	\includegraphics[width=\linewidth]{lungloss.png}
	\caption{\textit{Lungs/NotLungs} Loss Function values}
	\label{fig:lungloss}
\end{figure}
\clearpage
\newpage

\begin{table}
\caption{\textit{Skull/Not Skull} Results}
\label{tab:SNSres}
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Dataset Size & Train Acc. (\%) & Train Loss & Validation Acc. (\%) & Validation Loss & \textbf{Prediction Acc. (\%)} \\
\hline
3.000         & 1                   & 2,42E-04   & 99,65                    & 0,0129          & \textbf{99,53}                    \\
4.000         & 99,97               & 0,0059     & 99,87                    & 0,0021          & \textbf{99,69}                    \\
5.000         & 1                   & 8,28E-05   & 99,79                    & 0,0137          & \textbf{99,65}                    \\
8.000         & 1                   & 3,27E-05   & 99,88                    & 0,0068          & \textbf{99,69}                    \\
10.000        & 1                   & 8,00E-05   & 99,70                    & 0,0022          & \textbf{99,69}                    \\
20.000        & 1                   & 0,0016     & 99,67                    & 0,0111          & \textbf{99,90}                    \\
\hline
\end{tabular}}
\end{table}

\subsection{Skull/NotSkull}
The results for this tests come out with the same structure as the ones before. The complete dataset is composed of 11.000 images labeled as \textit{"Skull"} and 89.000 images labeled as \textit{"NotSkull"}. The difference between these two is even bigger than the one observed in the \textit{Lungs/NotLungs} set, due to the disparity in size between skull and lungs, the latter being clearly more dimensionally relevant. The methods used to obtain these are the same as before.
Once again, \textbf{Train, Validation and Prediction Accuracy} proportionally rise with the number of images in the Dataset used and \textbf{Train and Validation Loss} decrease with it. The overall higher accuracy of this test compared with the other one can be easily explained: skulls, in CT scans, are extremely different from other body parts. On the other side lungs, being in the thoracic cage, have a bigger risk of being misinterpreted as other anatomic sites.\ref{fig:ctcomparison} \\

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{ctcomparison.png}
	\caption{(A)Skull Section (B)Lungs Section (C)Abdomen Section}
	\label{fig:ctcomparison}
\end{figure}

%\begin{figure}[h!b]
%	\centering
%	\includegraphics[width=\linewidth]{skullacc.png}
%	\caption{\textit{Skull/NotSkull} Accuracy values}
%	\label{fig:skullacc}
%\end{figure}
\vspace*{1mm}
Even though for most part the results are exceptional and coherent, there are some particular situations which are present in this tests which have to be addressed: First of all the 4000 images Dataset, not only has the lowest \textit{Training Accuracy} but, more importantly, shows a \textit{Validation Loss} lower than the relative \textit{Train Loss}: due to this only happening in such Dataset and not presenting anymore it is possible to appoint it to some error during the setup which does not affect the test as a whole and can be almost completely ignored. \textit{(Figure \ref{fig:skulloss})} \\ The sudden raise of the validation loss when using more images might be sign of a beginning of model overfitting: considering the astonishing results on prediction set, it would be safe to further increase the Dataset size.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{skullloss.png}
	\caption{\textit{Skull/NotSkull} Loss Function values}
	\label{fig:skulloss}
\end{figure}
\clearpage
\newpage
\vspace*{1mm}
It also has to be noticed how the expected \textit{Prediction Accuracy} increase was basically not present in the 4.000 to 10.000 datasets, whilst appearing with a relatively high augmentation in the 20.000 images dataset. \textit{(Figure \ref{fig:skullacc})}.\\
This confirms what was also stated before, so that the one-time huge cost to train a CNN with as much data as possible is repaid by a better accuracy. Even in this different section, indeed, we could confirm the biggest dataset (20.000 images) being the \textbf{best} one. 
\vspace*{5mm}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{skullacc.png}
	\caption{\textit{Skull/NotSkull} Accuracy values}
	\label{fig:skullacc}
\end{figure}
\clearpage
\newpage

\begin{figure}[H]
   \centering
    \begin{subfigure}{0.4\linewidth}
        \includegraphics[width=\linewidth]{lungerror.png}
        %\caption{Lungs/NotLungs classification error (1)}
    \end{subfigure}
    \begin{subfigure}{0.4\linewidth}
        \includegraphics[width=\linewidth]{lerror.png}
        %\caption{Lungs/NotLungs classification error (2)}
    \end{subfigure}
   
  \caption{Examples of mistaken classified inputs}
  \label{fig:err}
\end{figure}

\section{Errors and Solution}
Even though the best \textbf{Prediction Accuracies} come out as \textbf{98.6\%} for \textit{Lungs/NotLungs} and an astonishing \textbf{99.9\%} for \textit{Skull/NotSkull}; they're still below the perfect \textit{100\%} mark which would ideally be the target. \\
A \textit{Python} script is used in order to identify which images are not being correctly identified and to understand the nature of the error. The possibility of the model \textbf{overfitting} is always present in machine learning and must be avoided at any cost. 
Focusing on the \textit{"Lungs/NotLungs"} Dataset, which is the one showing more errors, it is possible to identify some images which are labeled as \textit{Lungs} but predicted as \textit{NotLungs}. \textit{(Figure \ref{fig:err})}: this would be the worst possible error, as it causes for important images belonging to the target section to be completely ignored, but what comes out from such an analysis is that all the wrongly predicted images are not \textit{real} errors of the model rather than labelling mistakes. \\
\clearpage
\newpage

Most of the time the shoulder blades and trachea sections, which have minimum if no trace at all of lungs in them, are labeled as belonging to the target section whilst they do not. \textit{(Figure \ref{fig:lungblade})} \\
The same might happen with the lower part of lungs, the one confining with the stomach. Thanks to such tests it is possible to assume that the errors are not depending on the CNN model used but rather don the \textbf{labelling} step. This is to be attributed to inexperience in medical image labelling: labels taken too wide \textit{(i.e. shoulder blades considered as lungs)} or too short \textit{(i.e. lungs' sections closer to the abdomen labeled as not lungs)} end up causing the aforenoticed errors. With images correctly labeled by professional doctors such a problem should not be present at all and it should then be possibile to try and achieve the desired perfect performance.

\vspace*{5mm}
\begin{figure}[h!b]
	\centering
	\includegraphics[width=0.6\linewidth]{lblade.png}
	\caption{Near shoulder blades lungs section}
	\label{fig:lungblade}
\end{figure}

\section{Conclusions}

\subsection{Comparisons}
A direct comparison with some recent systems used for such task is not possible, since no standard medical dataset is available that can be used to benchmark the retrieval system,\cite{Qayyum2017} and the few other papers concerning automatic body-part recognition focus on multi-class classification tasks, whilst the here presented project only focuses on two classes at time representing the target section and its counterpart.\\
Considering all the papers describing other deep-learning methods there is one crucial trait to be addressed: A prediction dataset to execute results validation it is not used (or even mentioned at all)\cite{Qayyum2017}, or rather only just very few images \textit{(two)} are used to draw conclusions\cite{Lakhani2018}. This is a very poor evaluation method because, as it has already been stated, fails to detect overfitting and also overvalues the prediction capability of the trained network. Examples of what just stated are \textit{Roth et al.}, which work on 4928 images (splitted in 80\% train - 20\% test) belonging to five different classes, achieving an accuracy of \textbf{94.1\%} by using a LeNet inspired architecture.\cite{Roth2015} Even though the proposed results happen to be optimal there is no mention of an additional third dataset other than training and test ones, so it is unclear whether such results will be similar on different inputs rather than the training ones. Also the already cited work by \textit{Qayyum et al.}, which uses a LeNet architecture as well, achieves an average 99.77\% accuracy on 7200 images (70\% train - 30\% test) belonging to 24 classes. Yet again, though, there is no mention of a third dataset to evaluate the obtained results on images never seen by the network.\cite{Qayyum2017}\\
Whilst instead considering \textit{"Traditional Methods"} some parameters have to be defined, being the ones to be used in feature extraction and classification. In their work \textit{Aboud et al.}\cite{Aboud2015} use different parameters to identify five different classes, firstly training different combination of parameters on 500 images and then evaluating on 250 images only the most promising ones \textit{(train accuracy at least 90\%).} The used feature extraction methods are:
\begin{itemize}
	\item Color Extraction
	\item Texture Extraction
	\item Histogram of Oriented Gradients \textit{(HOG)}
\end{itemize}
As already said, the most promising training set are then classified with usage of \textbf{k-Nearest Neighbour} method: The best obtained result on a testing set with the aforemetioned process comes at 73\%, way lower than any deep-learning method.
\vspace{5mm}
\subsection{Conclusions and Future Works}
Training via transfer learning an high-performing CNN such as the used \textbf{GoogleNet InceptionV3} proves to be extremely effective on a binary body-part classification task (with less images needed when the two classes are very different from each other \ref{tab:SNSres}), being completely able to compare with other \textit{State of the Art} classifiers. A possible future work could include merging different binary trained models for each desired section \textit{(i.e.: Skull/NotSkull + Lungs/NotLungs + Liver/NotLiver + Legs/NotLegs, etc.)}. \\
The resulting multi-structured architecture might prove to outperform the current State of Art, consisting in multi-classifying LeNets.

\end{document}