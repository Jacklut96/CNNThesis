\documentclass[../main.tex]{subfiles}
\begin{document}
\thispagestyle{empty}
\section{Dataset}

Our dataset of images (about 100,000) is composed from a private dataset, which, for privacy reasons, is not possible to cite, and from a public dataset (www.cancerimagingarchive.net) \cite{Clark2013,QIN,data}.
CT images included are of the whole body of patients, from head to toe. Since the preliminary phase of our project is aimed at the first recognition of lung / not-lung from a slice, in order to be able to train neural networks, we had to select manually slices corresponding to lung.
These images before being processed by the neural network have been improved, to make them more readable by the network.


\subsection{Image Data Pre-Processing}
Building an effective neural network model requires careful consideration of the network architecture as well as the input data format. The most common image data input parameters are the number of images, image height, image width, number of channels and number of levels per pixel. Typically we have 3 channels of data corresponding to the colors Red,Green, Blue (RGB) Pixel levels are usually [0,255].

Dataset images need to be converted into the right format, we converted them in \textit{.csv} file to reduce variables reading time. After downloading the image data, images are arranged in separate sub-folders by patient's ID and again in lung/not lung folders.
The paragraphs below list pre-procesing steps need to apply to dataset.

\textbf{Uniform aspect ratio:} One of the first steps is to ensure that the images have the same size and aspect ratio. For example we wnat a square shape input image, which means that each image need to be checked if it is a square or not, and cropped appropriately. 

\textbf{Image Scaling:} Once we’ve ensured that all images have some predetermined aspect ratio, it’s time to scale each image appropriately. We decided to have images with width and height of 128x128 pixel, approximately 3 GB RAM required instead of the initially 52 GB RAM of the 512x512 pixel images. We can easily see that this step is necessary to relieve the load on the software. To do this we can use already existent library functions.

\textbf{Normalizing image inputs:} Data normalization is an important step which ensures that pixels has a similar data distribution. This makes convergence faster while training the network. Data normalization is done by subtracting the mean from each pixel, and then dividing the result by the standard deviation. The distribution of such data would resemble a Gaussian curve centered at zero. For image inputs we need the pixel numbers to be positive, so we might choose to scale the normalized data in the range [0,1] or [0, 255]. 

\subsection{Overfitting: training issue and solution}
Once the dataset has been treated we started with the training set. We used 80\% of images, randomly selected, for training set, and the following 20\% for testing set.
In this phase of the project we had to face and solve one of the problems that occur during neural network training: overfitting (we said that training data were \textit{too well} modelled). The error on the training set is driven to a very small value, but when new data is presented to the network the error is large. The network has memorized the training examples, but it has not learned to generalize to new situations.
One method to avoid this is to use a network that is just large enough to provide an adequate fit. The larger network you use, the more complex the functions the network can create. If you use a small enough network, it will not have enough power to overfit the data. Unfortunately, it is difficult to know beforehand how large a network should be for a specific application. 

So to avoid this issue the best solution seems to be applying a dropout layer to the net.
Dropout is a technique that prevents overfitting and provides a way of approximately combining exponentially many different neural network architectures efficiently. The term "dropout" refers to dropping out units (hidden and visible) in a neural network. By dropping a unit out, we mean temporarily removing it from the network, along with all its incoming and outgoing connections, as shown in Figure \ref{dropout}.

\begin{figure}[htbp] 
\centering 
\includegraphics[width=0.5\textwidth]{dropout}
\caption{Dropout Neural Net Model. \textbf{a)} A standard neural net with 2 hidden layers. \textbf{b)} An example of a thinned net produced by applying dropout to the network on the left.
Crossed units have been dropped.} 
\label{dropout} 
\vspace{5mm}
\end{figure}

The choice of which units to drop is random. In the simplest case, each unit is retained with a fixed probability p independent of other units, where p can be chosen using a validation set or can simply be set at 0.5, which seems to be close to optimal for a wide range of networks and tasks. Applying dropout to a neural network amounts to sampling a "thinned" network from
it. The thinned network consists of all the units that survived dropout (Figure \ref{dropout} b). A neural net with n units, can be seen as a collection of \(2^n\) possible thinned neural networks. So training a neural network with dropout can be seen as training a collection of \(2^n\) thinned networks with extensive weight sharing, where each thinned network gets trained
very rarely, if at all.
At test time, it is not feasible to explicitly average the predictions from exponentially
many thinned models. However, a very simple approximate averaging method works well in
practice. The idea is to use a single neural net at test time without dropout. The weights
of this network are scaled-down versions of the trained weights. If a unit is retained with
probability p during training, the outgoing weights of that unit are multiplied by p at test
time as shown in Figure \ref{probabilityp}.

\begin{figure}[htbp] 
\centering 
\includegraphics[width=0.7\textwidth]{probabilityp}
\caption{\textbf{a)} A unit at training time that is present with probability p and is connected to units
in the next layer with weights w. \textbf{b)} At test time, the unit is always present and the weights are multiplied by p. The output at test time is same as the expected output
at training time.} 
\label{probabilityp} 
\vspace{5mm}
\end{figure}

This ensures that for any hidden unit the expected output (under
the distribution used to drop units at training time) is the same as the actual output at
test time. By doing this scaling, \(2^n\) networks with shared weights can be combined into
a single neural network to be used at test time. Training a network with
dropout and using this approximate averaging method at test time leads to significantly
lower generalization error on a wide variety of classification problems compared to training
with other regularization methods.
\cite{overfitting,JMLR:v15:srivastava14a}


\section{Neural Networks}
The starting approach we used to find out the best network that feeds our needs, such as better accuracy, less information loss, and accessible processing time, was a trial and error approach. In the following section we'll show the neural nets we have considered and finally in the results we'll discuss the network we have definitely decided to use and what changes have been made to improve accuracy and performance.


DA FINIRE...
Feature extraction involves extracting a higher level of information from raw pixel values that can capture the distinction among the categories involved. This feature extraction is done in an unsupervised manner wherein the classes of the image have nothing to do with information extracted from pixels. After the feature is extracted, a classification module is trained with the images and their associated labels.

The philosophy behind deep learning is correlated with human being. Just after birth, a child is incapable of perceiving his surroundings, but as he progresses and processes data, he learns to identify things, wherein no hard-coded feature extractor is built in. It combines the extraction and classification modules into one integrated system and it learns to extract, by discriminating representations from the images and classify them based on supervised data.

\subsection{Introduction}
\textbf{Artificial neural networks (ANNs)} are software implementations of the neuronal
structure of our brains, it contains neurons which are kind of like organic
switches. These can change their output state depending on the strength of their
electrical or chemical input. The neural network in a person’s brain is a hugely
interconnected network of neurons, where the output of any given neuron may be the
input to thousands of other neurons. Learning occurs by repeatedly activating certain
neural connections over others, and this reinforces those connections. This
makes them more likely to produce a desired outcome given a specified input. This
learning involves feedback – when the desired outcome occurs, the neural connections
causing that outcome becomes strengthened. Artificial neural networks (ANN) attempt to simplify and mimic this brain behavior. They can be
trained in a supervised or unsupervised manner. In a supervised ANN, the network is
trained by providing matched input and output data samples, with the intention of getting
the ANN to provide a desired output for a given input. The learning takes place be adjusting the weights of the ANN connections. \cite

Since an image consists of many pixels and each pixel is possibly represented by multiple color values, the representation of that image in the input layer can become highly complex. For example, a full HD RGB image with 1920×1080 pixels would, for instance, require an input layer consisting of about six million neurons. If one would use
the simple fully-connected network architecture, each neuron in the subsequent layer would then
be connected to about six million neurons and if the first
fully-connected layer would contain just 1000 neurons, the
total number of parameters would amount to over six billion.
Since the network has to optimize all of these parameters,
the training process could then become very time and storage intensive.

In order to solve the computational problem, a different
kind of network architecture is used, called \textbf{Convolutional Neural Network (CNN)}. CNNs are specifically designed for
working with images. For this reason, the neurons of a layer
are organized across the three dimensions, height, width and
depth, just like the pixels in an image where the depth
dimension would differentiate the different color values. In
addition to that, CNNs introduce two new types of hidden
layers in which each neuron is only connected to a small
subset of the neurons in the previous layer, to prevent the
aforementioned problem. \cite{article}

The main aspects we have to consider to have a good convolutional neural network are:

\textbf{1. Accuracy:}
It is absolutely critical that it must be as accurate as possible. Accuracy not only depends on the network but also on the amount of data available for training. Bigger is the dataset available, better the accuracy will be.

\textbf{b. Computation:}
Most Convolutional Neural Networks (CNN) have huge memory and computation requirements, especially while training. Hence, this becomes an important concern. Similarly, the size of the final trained model becomes important to consider if you are looking to deploy a model to run locally on mobile. As you can guess, it takes a more computationally intensive network to produce more accuracy. So, there is always a trade-off between accuracy and computation.

There are also other factors like ease of training and the ability of a network to generalize well. The networks described below are the famous ones and the ones we used for our test, we'll next comment preliminary grapichs that show results from using different nets.

\subsection{LeNet}
LeNet-5 comprises 7 layers, not counting the input, all of which contain trainable parameters, called weights.
\begin{figure}[htbp] 
\centering 
\includegraphics[width=0.5\textwidth]{lenet}
\caption{LeNet architecture} 
\label{lenet} 
\end{figure}

\cite{Lecun}

\subsection{VGGNet}
This architecture is characterized by its simplicity, less levels but more convolutions each, using only 3×3 convolutional layers stacked on top of each other in increasing depth. Reducing volume size is handled by max pooling. Two fully-connected layers are then followed by a softmax classifier.

\begin{figure}[htbp] 
\centering 
\includegraphics{vggnet}
\caption{VGGNet architecture \cite{vggnet}} 
\label{vggnet} 
\end{figure} 

Each convolutional layer uses ReLu(Rectified Linear Unit) for the non-linear part instead of a Sigmoid function which was the earlier standard for traditional neural networks. ReLu is given by 

f(x) = max(0,x)

The advantage of the ReLu over sigmoid is that it trains much faster than the latter because the derivative of sigmoid becomes very small in the saturating region and therefore the updates to the weights almost vanish(Figure 4). This is called vanishing gradient problem.\\
In the network, ReLu layer is put after each and every convolutional and fully-connected layers(FC).

\begin{figure}[htbp] 
\centering 
\includegraphics[width=0.5\textwidth]{relu}
\caption{ReLu} 
\label{relu} 
\end{figure}

Despite VGGNet being characterized by its simplicity and its ability in solving many deep learning image classification problems, this network has some disadvantages: it is very slow to train and the network architecture weights are quite large.\cite{vgg}

\subsection{VGGNet vs Lenet}

\vspace{5mm}
\end{document}