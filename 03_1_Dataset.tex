\documentclass[../main.tex]{subfiles}
\begin{document}
\thispagestyle{empty}
\section{Dataset}

Our dataset of images (about 100,000) is composed from a private dataset, which, for privacy reasons, is not possible to cite, and from a public dataset (www.cancerimagingarchive.net) \cite{Clark2013,QIN,data}.
CT images included are of the whole body of patients, from head to toe. Since the preliminary phase of our project is aimed at the first recognition of lung / not-lung from a slice, in order to be able to train neural networks, we had to select manually slices corresponding to lung.
These images before being processed by the neural network have been improved, to make them more readable by the network.


\subsection{Image Data Pre-Processing}
%As we know deep learning uses neural nets with a lot of hidden layers and requires large amounts of training data. Thanks to availability of huge data-sets and advances in raw compute power and the rise of efficient parallel hardware the use of machine learning in the recent years has increased steadily.
Building an effective neural network model requires careful consideration of the network architecture as well as the input data format. The most common image data input parameters are the number of images, image height, image width, number of channels and number of levels per pixel. Typically we have 3 channels of data corresponding to the colors Red,Green, Blue (RGB) Pixel levels are usually [0,255].

Dataset images need to be converted into the right format, we converted them in \textit{.csv} file to reduce variables reading time. After downloading the image data, images are arranged in separate sub-folders by patient's ID and again in lung/not lung folders.
The paragraphs below list pre-procesing steps need to apply to dataset.

\textbf{Uniform aspect ratio:} One of the first steps is to ensure that the images have the same size and aspect ratio. For example we wnat a square shape input image, which means that each image need to be checked if it is a square or not, and cropped appropriately. 

\textbf{Image Scaling:} Once we’ve ensured that all images have some predetermined aspect ratio, it’s time to scale each image appropriately. We decided to have images with width and height of 128x128 pixel, approximately 3 GB RAM required instead of the initially 52 GB RAM of the 512x512 pixel images. We can easily see that this step is necessary to relieve the load on the software. To do this we can use already existent library functions.

\textbf{Normalizing image inputs:} Data normalization is an important step which ensures that pixels has a similar data distribution. This makes convergence faster while training the network. Data normalization is done by subtracting the mean from each pixel, and then dividing the result by the standard deviation. The distribution of such data would resemble a Gaussian curve centered at zero. For image inputs we need the pixel numbers to be positive, so we might choose to scale the normalized data in the range [0,1] or [0, 255]. 

\subsection{Overfitting: training issue and solution}
Once the dataset has been treated we started with the training set. We used 80\% of images, randomly selected, for training set, and the following 20\% for testing set.
In this phase of the project we had to face and solve one of the problems that occur during neural network training: overfitting (we said that training data were \textit{too well} modelled). The error on the training set is driven to a very small value, but when new data is presented to the network the error is large. The network has memorized the training examples, but it has not learned to generalize to new situations.
One method to avoid this is to use a network that is just large enough to provide an adequate fit. The larger network you use, the more complex the functions the network can create. If you use a small enough network, it will not have enough power to overfit the data. Unfortunately, it is difficult to know beforehand how large a network should be for a specific application. 

So to avoid this issue the best solution seems to be applying a dropout layer to the net.
Dropout is a technique that prevents overfitting and provides a way of approximately combining exponentially many different neural network architectures efficiently. The term "dropout" refers to dropping out units (hidden and visible) in a neural network. By dropping a unit out, we mean temporarily removing it from the network, along with all its incoming and outgoing connections, as shown in Figure \ref{dropout}.

\begin{figure}[htbp] 
\centering 
\includegraphics[width=0.5\textwidth]{dropout}
\caption{Dropout Neural Net Model. \textbf{a)} A standard neural net with 2 hidden layers. \textbf{b)} An example of a thinned net produced by applying dropout to the network on the left.
Crossed units have been dropped.} 
\label{dropout} 
\vspace{5mm}
\end{figure}

The choice of which units to drop is random. In the simplest case, each unit is retained with a fixed probability p independent of other units, where p can be chosen using a validation set or can simply be set at 0.5, which seems to be close to optimal for a wide range of networks and tasks. Applying dropout to a neural network amounts to sampling a "thinned" network from
it. The thinned network consists of all the units that survived dropout (Figure \ref{dropout} b). A neural net with n units, can be seen as a collection of \(2^n\) possible thinned neural networks. So training a neural network with dropout can be seen as training a collection of \(2^n\) thinned networks with extensive weight sharing, where each thinned network gets trained
very rarely, if at all.
At test time, it is not feasible to explicitly average the predictions from exponentially
many thinned models. However, a very simple approximate averaging method works well in
practice. The idea is to use a single neural net at test time without dropout. The weights
of this network are scaled-down versions of the trained weights. If a unit is retained with
probability p during training, the outgoing weights of that unit are multiplied by p at test
time as shown in Figure \ref{probabilityp}.

\begin{figure}[htbp] 
\centering 
\includegraphics[width=0.7\textwidth]{probabilityp}
\caption{\textbf{a)} A unit at training time that is present with probability p and is connected to units
in the next layer with weights w. \textbf{b)} At test time, the unit is always present and the weights are multiplied by p. The output at test time is same as the expected output
at training time.} 
\label{probabilityp} 
\vspace{5mm}
\end{figure}

This ensures that for any hidden unit the expected output (under
the distribution used to drop units at training time) is the same as the actual output at
test time. By doing this scaling, \(2^n\) networks with shared weights can be combined into
a single neural network to be used at test time. Training a network with
dropout and using this approximate averaging method at test time leads to significantly
lower generalization error on a wide variety of classification problems compared to training
with other regularization methods.
\cite{overfitting,JMLR:v15:srivastava14a}


\section{Neural Networks}
The starting approach we used to find out the best network that feeds our needs, such as better accuracy, less information loss, and accessible processing time, was a trial and error approach. In the following section we'll show the neural nets we have considered and finally in the results we'll discuss the network we have definitely decided to use and what changes have been made to improve accuracy and performance.


DA FINIRE...
Feature extraction involves extracting a higher level of information from raw pixel values that can capture the distinction among the categories involved. This feature extraction is done in an unsupervised manner wherein the classes of the image have nothing to do with information extracted from pixels. After the feature is extracted, a classification module is trained with the images and their associated labels.

The philosophy behind deep learning is correlated with human being. Just after birth, a child is incapable of perceiving his surroundings, but as he progresses and processes data, he learns to identify things, wherein no hard-coded feature extractor is built in. It combines the extraction and classification modules into one integrated system and it learns to extract, by discriminating representations from the images and classify them based on supervised data.

One such system is multilayer perceptrons aka neural networks which are multiple layers of neurons densely connected to each other. A deep vanilla neural network has such a large number of parameters involved that it is impossible to train such a system without overfitting the model due to the lack of a sufficient number of training examples. But with Convolutional Neural Networks(ConvNets), the task of training the whole network from the scratch can be carried out using a large dataset like ImageNet. The reason behind this is, sharing of parameters between the neurons and sparse connections in convolutional layers. It can be seen in this figure 2. In the convolution operation, the neurons in one layer are only locally connected to the input neurons and the set of parameters are shared across the 2-D feature map.
IMMAGINE

a. Accuracy:
If you are building an intelligent machine, it is absolutely critical that it must be as accurate as possible. One fair question to ask here is that ‘accuracy not only depends on the network but also on the amount of data available for training’. Hence, these networks are compared on a standard dataset called ImageNet.

b. Computation:
Most ConvNets have huge memory and computation requirements, especially while training. Hence, this becomes an important concern. Similarly, the size of the final trained model becomes important to consider if you are looking to deploy a model to run locally on mobile. As you can guess, it takes a more computationally intensive network to produce more accuracy. So, there is always a trade-off between accuracy and computation.

Apart from these, there are many other factors like ease of training, the ability of a network to generalize well etc. The networks described below are the most popular ones and are presented in the order that they were published and also had increasingly better accuracy from the earlier ones.

\subsection{AlexNet}
This architecture was one of the first deep networks to push ImageNet Classification accuracy by a significant stride in comparison to traditional methodologies. It is composed of 5 convolutional layers followed by 3 fully connected layers, as depicted in Figure 1.
\begin{figure}[htbp] 
\centering 
\includegraphics[width=0.5\textwidth]{alexnet}
\caption{AlexNet} 
\label{alexnet} 
\end{figure} 

AlexNet, proposed by Alex Krizhevsky, uses ReLu(Rectified Linear Unit) for the non-linear part, instead of a Tanh or Sigmoid function which was the earlier standard for traditional neural networks. ReLu is given by 

f(x) = max(0,x)

The advantage of the ReLu over sigmoid is that it trains much faster than the latter because the derivative of sigmoid becomes very small in the saturating region and therefore the updates to the weights almost vanish(Figure 4). This is called vanishing gradient problem.\\
In the network, ReLu layer is put after each and every convolutional and fully-connected layers(FC).

\begin{figure}[htbp] 
\centering 
\includegraphics[width=0.5\textwidth]{relu}
\caption{ReLu} 
\label{relu} 
\end{figure}
Another problem that this architecture solved was reducing the over-fitting by using a Dropout layer after every FC layer. Dropout layer has a probability,(p), associated with it and is applied at every neuron of the response map separately. It randomly switches off the activation with the probability p, as can be seen in the figure below.  

\begin{figure}[htbp] 
\centering 
\includegraphics[width=0.5\textwidth]{dropout}
\caption{Dropout} 
\label{dropout} 
\end{figure}
The idea behind the dropout is similar to the model ensembles. Due to the dropout layer, different sets of neurons which are switched off, represent a different architecture and all these different architectures are trained in parallel with weight given to each subset and the summation of weights being one. For n neurons attached to DropOut, the number of subset architectures formed is 2^n. So it amounts to prediction being averaged over these ensembles of models. This provides a structured model regularization which helps in avoiding the over-fitting. Another view of DropOut being helpful is that since neurons are randomly chosen, they tend to avoid developing co-adaptations among themselves thereby enabling them to develop meaningful features, independent of others.

\subsection{GoogLeNet/Inception}
The GoogLeNet builds on the idea that most of the activations in a deep network are either unnecessary(value of zero) or redundant because of correlations between them. Therefore the most efficient architecture of a deep network will have a sparse connection between the activations, which implies that all 512 output channels will not have a connection with all the 512 input channels. There are techniques to prune out such connections which would result in a sparse weight/connection. But kernels for sparse matrix multiplication are not optimized in BLAS or CuBlas(CUDA for GPU) packages which render them to be even slower than their dense counterparts.

\begin{figure}[htbp] 
\centering 
\includegraphics[width=0.5\textwidth]{googlenet}
\caption{GoogleNet} 
\label{https://stackoverflow.com/questions/47633393/how-to-calculate-the-number-of-layer-for-googlenet} 
\end{figure}
So GoogLeNet devised a module called inception module that approximates a sparse CNN with a normal dense construction(shown in the figure). Since only a small number of neurons are effective as mentioned earlier, the width/number of the convolutional filters of a particular kernel size is kept small. Also, it uses convolutions of different sizes to capture details at varied scales(5X5, 3X3, 1X1).

Another salient point about the module is that it has a so-called bottleneck layer(1X1 convolutions in the figure). It helps in the massive reduction of the computation requirement as explained below.

Let us take the first inception module of GoogLeNet as an example which has 192 channels as input. It has just 128 filters of 3X3 kernel size and 32 filters of 5X5 size. The order of computation for 5X5 filters is 25X32X192 which can blow up as we go deeper into the network when the width of the network and the number of 5X5 filter further increases. In order to avoid this, the inception module uses 1X1 convolutions before applying larger sized kernels to reduce the dimension of the input channels, before feeding into those convolutions. So in the first inception module, the input to the module is first fed into 1X1 convolutions with just 16 filters before it is fed into 5X5 convolutions. This reduces the computations to 16X192 +  25X32X16. All these changes allow the network to have a large width and depth.

Another change that GoogLeNet made, was to replace the fully-connected layers at the end with a simple global average pooling which averages out the channel values across the 2D feature map, after the last convolutional layer. This drastically reduces the total number of parameters. This can be understood from AlexNet, where FC layers contain approx. 90/100 of parameters. Use of a large network width and depth allows GoogLeNet to remove the FC layers without affecting the accuracy.
\cite{multi}
\vspace{5mm}
\end{document}