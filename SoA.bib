Automatically generated by Mendeley Desktop 1.19.1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{Tensorflow2017,
author = {Tensorflow},
file = {:C$\backslash$:/Users/tauro/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/25e1637fe7adb2beb5804e9e322a0a57f36ec370.html:html},
title = {{Index @ Www.Tensorflow.Org}},
url = {http://www.tensorflow.org/tutorials/mnist/beginners/index.md},
year = {2017}
}
@article{Russakovsky,
archivePrefix = {arXiv},
arxivId = {arXiv:1409.0575v3},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Jan, C V and Krause, J and Ma, S},
eprint = {arXiv:1409.0575v3},
keywords = {benchmark,dataset,large-scale,object detection,object recognition},
title = {{ImageNet Large Scale Visual Recognition Challenge}}
}
@article{Punithavathy2015,
author = {Punithavathy, K and Ramya, M.M. and Poobal, Sumathi},
doi = {10.1109/RACE.2015.7097244},
file = {::},
isbn = {978-8-1925-9743-0},
journal = {International Conference on Robotics, Automation, Control and Embedded Systems - RACE},
keywords = {automatic detection,ct,lung cancer,pet},
number = {February},
pages = {14--18},
title = {{Analysis of Statistical Texture Features for Automatic Lung Cancer Detection in PET / CT Images}},
year = {2015}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {::},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
pages = {1--9},
pmid = {7491034},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{Amutha2013,
abstract = {In recent years, the image processing mechanisms are widely used in medical image diagnosis, especially in detection of various tumors. In this paper, we propose a level set-active contour model with minimizer function for lung tumor diagnosis and segmentation. Kernel based non-local neighborhood denoising function is used to get noise free image. Second order histogram based feature extraction is accomplished for classifying the images under normal and abnormal classes. Following tumor detection, exact segmentation of the tumor is effected by level set-active contour modeling with minimized gradient. Experiments demonstrated that our methodology could segment the lung field with pathology of variant forms more precisely. I.},
author = {Amutha, A and Wahidabanu, R S D},
file = {::},
isbn = {9781467348669},
keywords = {ct,enhancement,feature extraction,segmentation},
number = {7},
pages = {1108--1112},
title = {{Lung Tumor Detection and Diagnosis in CT scan Images}},
volume = {6},
year = {2013}
}
@article{Guresen2011,
abstract = {Definition of Artificial Neural Networks (ANNs) is made by computer scientists, artificial intelligence experts and mathematicians in various dimensions. Many of the definitions explain ANN by referring to graphics instead of giving well explained mathematical definitions; therefore, misleading weighted graphs (as in minimum cost flow problem networks) fit the definition of ANN. This study aims to give a clear definition that will differentiate ANN and graphical networks by referring to biological neural networks. The proposed definition of ANN is a mathematical definition, from the point of graph theory which defines ANN as a directed graph. Then differences between ANNs and other networks will be explained by examples using proposed definition. {\textcopyright} 2010 Published by Elsevier Ltd.},
author = {Guresen, Erkam and Kayakutlu, Gulgun},
doi = {10.1016/j.procs.2010.12.071},
file = {::},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Artificial Neural Network (ANN),Graph theory},
pages = {426--433},
publisher = {Elsevier},
title = {{Definition of Artificial Neural Networks with comparison to other networks}},
url = {http://dx.doi.org/10.1016/j.procs.2010.12.071},
volume = {3},
year = {2011}
}
@article{LeCun1998,
abstract = {A long and detailed paper on convolutional nets, graph transformer$\backslash$nnetworks, and discriminative training methods for sequence labeling.$\backslash$nWe show how to build systems that integrate segmentation, feature$\backslash$nextraction, classification, contextual post-processing, and language$\backslash$nmodeling into one single learning machine trained end-to-end. Applications$\backslash$nto handwriting recognition and face detection are described.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {LeCun, Yann and Bottou, Leon and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
eprint = {1102.0183},
file = {::},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {character recognition,convolutional neural networks,document recog-,finite state transducers,gradient-based learning,graph,machine learning,neural networks,nition,ocr,optical,transformer networks},
number = {11},
pages = {2278--2324},
pmid = {15823584},
title = {{Gradient Based Learning Applied to Document Recognition}},
url = {http://ieeexplore.ieee.org/abstract/document/726791/},
volume = {86},
year = {1998}
}
@inproceedings{Simonyan2015,
abstract = {Sentiment prediction from visual content is a challenge due to the difficulty of inferring sentiment directly from the low-level visual features. Most recent researches use Adjective Noun Pairs (ANPs) as a middle level to narrow the gap between vision and sentiment. While as Convolutional Neural Networks (CNNs) is going deeper, it is becoming possible to implement rather complex mappings. In this paper, an image dataset with sentiment tags is built for training. We conduct the experiment by training 15,000 scene images on three different CNNs models, proving that deep learning can perform rather well on specific sentiment prediction task. CNNs models are also more concise and easy than those using ANPs in the aspect of data collection and engineering implementation. {\textcopyright} 2016 IEEE.},
archivePrefix = {arXiv},
arxivId = {arXiv:1409.1556v6},
author = {Simonyan, Karen and Zisserman, Andrew},
booktitle = {ICLR 2015},
doi = {10.1109/YAC.2016.7804888},
eprint = {arXiv:1409.1556v6},
file = {::},
isbn = {9781509044238},
keywords = {convolutional neural networks,sentiment prediction,visual features},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
year = {2015}
}
@article{Svozil1997,
abstract = {Basic definitions concerning the multi-layer feed-forward neural networks are given. The back-propagation training algorithm is explained. Partial derivatives of the objective function with respect to the weight and threshold coefficients are derived. These derivatives are valuable for an adaptation process of the considered neural network. Training and generalisation of multi-layer feed-forward neural networks are discussed. Improvements of the standard back-propagation algorithm are reviewed. Example of the use of multi-layer feed-forward neural networks for prediction of carbon-13 NMR chemical shifts of alkanes is given. Further applications of neural networks in chemistry are reviewed. Advantages and disadvantages of multilayer feed-forward neural networks are discussed.},
author = {Svozil, Daniel and Kvasnicka, Vladim{\'{i}}r and Pospichal, JirÌ‚{\'{i}}},
doi = {10.1016/S0169-7439(97)00061-0},
file = {::},
isbn = {0169-7439},
issn = {0169-7439},
journal = {Chemometrics and Intelligent Laboratory Systems},
keywords = {neural networks},
number = {1},
pages = {43--62},
title = {{Introduction to multi-layer feed-forward neural networks}},
url = {https://www.sciencedirect.com/science/article/pii/S0169743997000610},
volume = {39},
year = {1997}
}
@misc{CancerResearchUK,
author = {{Cancer Research UK}},
file = {:C$\backslash$:/Users/tauro/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/3eb2d1b1ac34a3dbde8572de1d1ad6625e107ea6.html:html},
title = {{Lung-Cancer @ Www.Cancerresearchuk.Org}},
url = {http://www.cancerresearchuk.org/health-professional/cancer-statistics/statistics-by-cancer-type/lung-cancer}
}
@article{AlMohammad2017,
abstract = {Lung cancer is the leading cause of cancer-related death worldwide; however, early diagnosis of lung cancer leads to higher survival rates. The National Lung Screening Trial (NLST) demonstrated that scanning with low-dose computed tomography (LDCT) led to a 20{\%} reduction in mortality rate in a high-risk population. This paper covers new developments in screening eligibility criteria and the possible benefits and the harm of screening with CT. To make the screening process more feasible and help reduce the rate of missed lung nodules, computer-aided detection (CAD) has been introduced to assist radiologists in lung nodule detection. The aim of this paper is to review how CAD works, its performance in lung nodule detection, and the factors that influence its performance. This paper also aims to investigate the effect of different types of CAD on CT in lung nodule detection and the effect of CAD on radiologists' decision outcomes.},
author = {{Al Mohammad}, B. and Brennan, P. C. and Mello-Thoms, C.},
doi = {10.1016/j.crad.2017.01.002},
file = {:C$\backslash$:/Users/tauro/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Al Mohammad, Brennan, Mello-Thoms - 2017 - A review of lung cancer screening and the role of computer-aided detection.pdf:pdf},
isbn = {1365-229X (Electronic)0009-9260 (Linking)},
issn = {1365229X},
journal = {Clinical Radiology},
number = {6},
pages = {433--442},
pmid = {28185635},
publisher = {The Royal College of Radiologists},
title = {{A review of lung cancer screening and the role of computer-aided detection}},
url = {http://dx.doi.org/10.1016/j.crad.2017.01.002},
volume = {72},
year = {2017}
}
@article{Chaudhary2012,
abstract = {Lung cancer seems to be the common cause of death among people throughout the world. Early detection of lung cancer can increase the chance of survival among people. The overall 5-year survival rate for lung cancer patients increases from 14 to 49{\%} if the disease is detected in time. Although Computed Tomography (CT) can be more efficient than X-ray. However, problem seemed to merge due to time constraint in detecting the present of lung cancer regarding on the several diagnosing method used. Hence, a lung cancer detection system using image processing is used to classify the present of lung cancer in an CT-images. In this study, MATLAB have been used through every procedures made. In image processing procedures, process such as image pre-processing, segmentation and feature extraction have been discussed in detail. We are aiming to get the more accurate results by using various enhancement and segmentation techniques.},
author = {Chaudhary, Anita and Singh, Sonit Sukhraj},
doi = {10.1109/ICCS.2012.43},
file = {::},
isbn = {978-1-4673-2647-6},
journal = {2012 International Conference on Computing Sciences},
keywords = {-ct,lcds,metastasis,morphologic,thresholding,watershed},
pages = {142--146},
title = {{Lung Cancer Detection on CT Images by Using Image Processing}},
url = {http://ieeexplore.ieee.org/document/6391662/},
year = {2012}
}
@article{LogeshKumar2016,
abstract = {The detection of lung cancer through image processing is an important tool for diagnoses. Different methods were employed to detect the cancerous cell of a lung with image pre processing such as Gabor filter, image segmentation using watershed segmentation and feature extraction by using MATLAB. In this experiment, various Computed Tomography (CT) images of lung were used as input image and obtained the output image in JPEG format. The resultant output of this technique was evaluated. Based on the results, it shows that watershed segmentation technique is showed better results than other segmentation techniques for lung cancer cell identification.},
author = {{Logesh Kumar}, S. and Swathy, M. and Sathish, S. and Sivaraman, J. and Rajasekar, M.},
doi = {10.17485/ijst/2016/v9i1/85765},
file = {::},
issn = {09745645},
journal = {Indian Journal of Science and Technology},
keywords = {CT lung cancer image,Image segmentation,Thresholding operation,Watershed method},
number = {1},
pages = {1--4},
title = {{Identification of lung cancer cell using watershed segmentation on CT images}},
volume = {9},
year = {2016}
}
@article{Canziani2016,
abstract = {Since the emergence of Deep Neural Networks (DNNs) as a prominent technique in the field of computer vision, the ImageNet classification challenge has played a major role in advancing the state-of-the-art. While accuracy figures have steadily increased, the resource utilisation of winning models has not been properly taken into account. In this work, we present a comprehensive analysis of important metrics in practical applications: accuracy, memory footprint, parameters, operations count, inference time and power consumption. Key findings are: (1) power consumption is independent of batch size and architecture; (2) accuracy and inference time are in a hyperbolic relationship; (3) energy constraint is an upper bound on the maximum achievable accuracy and model complexity; (4) the number of operations is a reliable estimate of the inference time. We believe our analysis provides a compelling set of information that helps design and engineer efficient DNNs.},
archivePrefix = {arXiv},
arxivId = {1605.07678},
author = {Canziani, Alfredo and Paszke, Adam and Culurciello, Eugenio},
eprint = {1605.07678},
file = {::},
isbn = {2857825749},
pages = {1--7},
title = {{An Analysis of Deep Neural Network Models for Practical Applications}},
url = {http://arxiv.org/abs/1605.07678},
year = {2016}
}
@article{Khan2015,
author = {Khan, Daud Hossain and Ahmed, Mansur and Bach, Christian},
file = {::},
journal = {Journal of Biomedical Engineering and Medical Imaging},
keywords = {area,ct,lung cancer,matlab,region-of-interest,roi},
number = {6},
pages = {1--7},
title = {{Preliminary Detection and Analysis of Lung Cancer on CT images using MATLAB : A Cost-effective Alternative}},
volume = {2},
year = {2015}
}
@article{Kuruvilla2014,
abstract = {Early detection of cancer is the most promising way to enhance a patient's chance for survival. This paper presents a computer aided classification method in computed tomography (CT) images of lungs developed using artificial neural network. The entire lung is segmented from the CT images and the parameters are calculated from the segmented image. The statistical parameters like mean, standard deviation, skewness, kurtosis, fifth central moment and sixth central moment are used for classification. The classification process is done by feed forward and feed forward back propagation neural networks. Compared to feed forward networks the feed forward back propagation network gives better classification. The parameter skewness gives the maximum classification accuracy. Among the already available thirteen training functions of back propagation neural network, the Traingdx function gives the maximum classification accuracy of 91.1{\%}. Two new training functions are proposed in this paper. The results show that the proposed training function 1 gives an accuracy of 93.3{\%}, specificity of 100{\%} and sensitivity of 91.4{\%} and a mean square error of 0.998. The proposed training function 2 gives a classification accuracy of 93.3{\%} and minimum mean square error of 0.0942. {\textcopyright} 2013 Elsevier Ireland Ltd.},
author = {Kuruvilla, Jinsa and Gunavathi, K.},
doi = {10.1016/j.cmpb.2013.10.011},
isbn = {1872-7565 (Electronic)$\backslash$r0169-2607 (Linking)},
issn = {01692607},
journal = {Computer Methods and Programs in Biomedicine},
keywords = {Computed tomography,Kurtosis,Neural network,Skewness},
number = {1},
pages = {202--209},
pmid = {24199657},
publisher = {Elsevier Ireland Ltd},
title = {{Lung cancer classification using neural networks for CT images}},
url = {http://dx.doi.org/10.1016/j.cmpb.2013.10.011},
volume = {113},
year = {2014}
}
@article{Hubel1962,
abstract = {What chiefly distinguishes cerebral cortex from other parts of the central nervous system is the great diversity of its cell types and inter- connexions. It would be astonishing if such a structure did not profoundly modify the response patterns of fibres coming into it. In the cat's visual cortex, the receptive field arrangements of single cells suggest that there is indeed a degree of complexity far exceeding anything yet seen at lower levels in the visual system.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hubel, David H and Wiesel, Torsten N},
doi = {10.1523/JNEUROSCI.1991-09.2009},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {0270-6474},
issn = {0022-3751},
journal = {Journal of Physiology},
number = {1},
pages = {106--154.2},
pmid = {19776262},
title = {{Receptive fields, binocular interaction and functional architecture in the cat's visual cortex}},
volume = {160},
year = {1962}
}
@misc{kerasinception,
author = {Chollet, Fran{\c{c}}ois},
file = {:C$\backslash$:/Users/tauro/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/28ef2b69b0b3d27882f87a7d6ff06346ae2cbcf8.html:html},
title = {{@Keras.io/Applications/InceptionV3}},
url = {https://keras.io/applications/{\#}inceptionv3},
year = {2018}
}
@misc{University,
author = {of Tartu, University},
title = {{Image Processing @ sisu.ut.ee}},
url = {https://sisu.ut.ee/imageprocessing/book/1},
urldate = {2018-08-01}
}
@article{Chen2017,
abstract = {Since the discovery of X-rays at the end of the 19(th) century, medical imageology has progressed for 100 years, and medical imaging has become an important auxiliary tool for clinical diagnosis. With the launch of the human genome project (HGP) and the development of various high-throughput detection techniques, disease exploration in the post-genome era has extended beyond investigations of structural changes to in-depth analyses of molecular abnormalities in tissues, organs and cells, on the basis of gene expression and epigenetics. These techniques have given rise to genomics, proteomics, metabolomics and other systems biology subspecialties, including radiogenomics. Radiogenomics is an important revolution in the traditional visually identifiable imaging technology and constitutes a new branch, radiomics. Radiomics is aimed at extracting quantitative imaging features automatically and developing models to predict lesion phenotypes in a non-invasive manner. Here, we summarize the advent and development of radiomics, the basic process and challenges in clinical practice, with a focus on applications in pulmonary nodule evaluations, including diagnostics, pathological and molecular classifications, treatment response assessments and prognostic predictions, especially in radiotherapy.},
author = {Chen, Bojiang and Zhang, Rui and Gan, Yuncui and Yang, Lan and Li, Weimin},
doi = {10.1186/s13014-017-0885-x},
file = {:C$\backslash$:/Users/tauro/Downloads/s13014-017-0885-x.pdf:pdf},
isbn = {1748-717X (Electronic) 1748-717X (Linking)},
issn = {1748717X},
journal = {Radiation Oncology},
keywords = {Lung cancer,Phenotype,Pulmonary nodule,Radiomics},
number = {1},
pages = {1--8},
pmid = {28915902},
publisher = {Radiation Oncology},
title = {{Development and clinical application of radiomics in lung cancer}},
volume = {12},
year = {2017}
}
@article{Wasserman2015,
author = {Wasserman, Hillary},
file = {:C$\backslash$:/Users/tauro/Downloads/2017-WCLC-Fact-Sheet-Lung-Cancer-Final.pdf:pdf},
journal = {American Cancer Society},
keywords = {American Cancer Society},
pages = {19--21},
title = {{Cancer Facts and Statistics}},
url = {http://www.cancer.org/research/cancerfactsstatistics/cancerfactsfigures2015/index},
year = {2015}
}
@article{Hussein2017,
abstract = {Characterization of lung nodules as benign or malignant is one of the most important tasks in lung cancer diagnosis, staging and treatment planning. While the variation in the appearance of the nodules remains large, there is a need for a fast and robust computer aided system. In this work, we propose an end-to-end trainable multi-view deep Convolutional Neural Network (CNN) for nodule characterization. First, we use median intensity projection to obtain a 2D patch corresponding to each dimension. The three images are then concatenated to form a tensor, where the images serve as different channels of the input image. In order to increase the number of training samples, we perform data augmentation by scaling, rotating and adding noise to the input image. The trained network is used to extract features from the input image followed by a Gaussian Process (GP) regression to obtain the malignancy score. We also empirically establish the significance of different high level nodule attributes such as calcification, sphericity and others for malignancy determination. These attributes are found to be complementary to the deep multi-view CNN features and a significant improvement over other methods is obtained.},
archivePrefix = {arXiv},
arxivId = {1703.00645},
author = {Hussein, Sarfaraz and Gillies, Robert and Cao, Kunlin and Song, Qi and Bagci, Ulas},
doi = {10.1109/ISBI.2017.7950686},
eprint = {1703.00645},
file = {:C$\backslash$:/Users/tauro/Downloads/drive-download-20180728T141600Z-001/1703.00645.pdf:pdf},
isbn = {9781509011711},
issn = {19458452},
journal = {Proceedings - International Symposium on Biomedical Imaging},
keywords = {Computed tomography,Computer-aided diagnosis,Deep learning,Lung cancer,Pulmonary nodule},
pages = {1007--1010},
title = {{TumorNet: Lung nodule characterization using multi-view Convolutional Neural Network with Gaussian Process}},
year = {2017}
}
@article{Lin2013,
abstract = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
archivePrefix = {arXiv},
arxivId = {1312.4400},
author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
doi = {10.1109/ASRU.2015.7404828},
eprint = {1312.4400},
file = {::},
isbn = {9781479972913},
issn = {03029743},
pages = {1--10},
pmid = {24356345},
title = {{Network In Network}},
url = {http://arxiv.org/abs/1312.4400},
year = {2013}
}
@article{Jawahar2014,
author = {Jawahar, C. V. and Shan, Shiguang},
doi = {10.1007/978-3-319-16634-6},
file = {:C$\backslash$:/Users/tauro/Downloads/943717.pdf:pdf},
isbn = {9783319166339},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {109--121},
title = {{Computer vision - ACCV 2014 workshops: Singapore, Singapore, november 1-2, 2014 revised selected papers, Part III}},
volume = {9010},
year = {2014}
}
@article{Gillies2016,
abstract = {radiomics In the past decade, the field of medical image analysis has grown exponentially, with an increased number of pattern recognition tools and an increase in data set sizes. These advances have facilitated the development of processes for high-throughput extraction of quantitative features that result in the conversion of images into mineable data and the subsequent analysis of these data for decision support; this practice is termed radiomics. This is in contrast to the traditional practice of treating medical images as pictures intended solely for visual interpretation. Radiomic data contain first-, second-, and higher-order statistics. These data are combined with other patient data and are mined with sophisticated bioinformatics tools to develop models that may potentially improve diagnostic, prognostic, and predictive accuracy. Because radiomics analyses are intended to be conducted with standard of care images, it is conceivable that conversion of digital images to mineable data will eventually become routine practice. This report describes the process of radiomics, its challenges, and its potential power to facilitate better clinical decision making, particularly in the care of patients with cancer.},
author = {Gillies, Robert J. and Kinahan, Paul E. and Hricak, Hedvig},
doi = {10.1148/radiol.2015151169},
file = {:C$\backslash$:/Users/tauro/Downloads/Radiomics{\_} images are more than pictures, they are data.pdf:pdf},
isbn = {0033-8419},
issn = {0033-8419},
journal = {Radiology},
number = {2},
pages = {563--577},
pmid = {26579733},
title = {{Radiomics: Images Are More than Pictures, They Are Data}},
url = {http://pubs.rsna.org/doi/10.1148/radiol.2015151169},
volume = {278},
year = {2016}
}
@article{Wang2009,
author = {Wang, Huiling},
doi = {10.1007/978-3-319-44781-0},
file = {::},
isbn = {9783319447810},
number = {November},
pages = {1--9},
title = {{Deep Convolutional Neural Networks for}},
year = {2009}
}
@article{Wu2017,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {Kaiming, He and Xiangyu, Zhang and Shaoqing, Ren and Jian, Sun},
doi = {10.1007/s11042-017-4440-4},
eprint = {1512.03385},
file = {::},
isbn = {978-1-4673-6964-0},
issn = {15737721},
journal = {Microsoft Research},
keywords = {Convolutional neural networks,Image steganalysis,Residual learning},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition}},
year = {2015}
}
@book{Indicators2017,
abstract = {This book includes key environmental indicators endorsed by OECD Environment Ministers and major environmental indicators from the OECD Core Set. These indicators reflect environmental progress made since the early 1990s and thus contribute to measuring environmental performance. Organised by issues such as climate change, air pollution, biodiversity, waste or water resources, they provide essential information for all those interested in the environment and in sustainable development.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Indicators, Oecd},
doi = {10.1787/health_glance-2017-en},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/tauro/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Indicators - 2017 - Health at a Glance 2017.pdf:pdf},
isbn = {9789264280397},
issn = {03044130},
pmid = {12870},
title = {{Health at a Glance 2017}},
url = {http://www.oecd-ilibrary.org/social-issues-migration-health/health-at-a-glance-2017{\_}health{\_}glance-2017-en},
year = {2017}
}
@article{Schmidhuber2015,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {Schmidhuber, J{\"{u}}rgen},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {1404.7828},
file = {::},
isbn = {0893-6080},
issn = {18792782},
journal = {Neural Networks},
keywords = {Deep learning,Evolutionary computation,Reinforcement learning,Supervised learning,Unsupervised learning},
pages = {85--117},
pmid = {25462637},
publisher = {Elsevier Ltd},
title = {{Deep Learning in neural networks: An overview}},
url = {http://dx.doi.org/10.1016/j.neunet.2014.09.003},
volume = {61},
year = {2015}
}
@article{Mu,
author = {Mu, Wei and Chen, Zhe},
file = {::},
title = {{Automated measurements of metabolic tumor volume and metabolic parameters in lung PET / CT imaging Automated measurements of metabolic tumor volume and metabolic parameters in lung PET / CT imaging}}
}
@article{Szegedy2015,
abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
file = {::},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1--9},
pmid = {24920543},
title = {{Going deeper with convolutions}},
volume = {07-12-June},
year = {2015}
}
@article{Szegedy2015a,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2{\%} top-1 and 5.6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5{\%} top-5 error on the validation set (3.6{\%} error on the test set) and 17.3{\%} top-1 error on the validation set.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
doi = {10.1109/CVPR.2016.308},
eprint = {1512.00567},
file = {:C$\backslash$:/Users/tauro/Downloads/1512.00567.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {08866236},
pmid = {8190083},
title = {{Rethinking the Inception Architecture for Computer Vision}},
url = {http://arxiv.org/abs/1512.00567},
year = {2015}
}
