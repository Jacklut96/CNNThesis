\documentclass[../main.tex]{subfiles}
\begin{document}
\label{capitolo5}
\thispagestyle{empty}

The first step in the construction of a Convolutional Neural Network is definining the system on which it will be built on. Furthermore a Dataset has to be defined an built, and only then it is possibile to perform training, testing and evaluations on the trained network in order to acquire useful data used to decide wether the built architecture is functional or not. All of the following operations have been executed on a %TODO: INSERIRE SPECIFICHE DWARF

\section{API}
\textbf{Tensorflow} is our library of choice. The free open source library developed by Google Brain is excellent for machine learning purposes\cite{Tensorflow2017}, and used in combination with Python it works as the backend of our Convolutional Neural Networks. \textbf{Keras} is our API of choice: Being an high-level API it allows us to make our code easier to read, also enhancing the speed at which the whole Network works. Without the use of Keras the whole network would require much more code and would also be slower. During a preliminar test with a pre-prepared dataset and two identical architectures the usage of Keras has shown to improve the timing by a huge margin \textit{(4 hours with only Tensorflow against 20 mins ca. with Keras'API)}. 

\section{Dataset}
The dataset is the collection of data which is used to train, test and validate the CNN. Our dataset is built by merging both private and public data, where the public data has been obtained from different sources all coming from \textit{Data Cancer Archive}, for a total of more than 90.000 different images.
The combination of different sources to build one single dataset is of extreme importance, as it helps the generalization of the whole system. Different images from different sources, taken from different machines with different settings, allow the CNN to have a better understanding of the possible natural variations of CT Images caused by different instrumentation.
All the images composing the dataset were originally greyscale dicom files \textit{(.dcm)}, but all had different dimensions and encoding, bringing forth the need of preprocessing.
\vspace{5mm}
\subsection{Preprocessing}
All the following processes are executed by means of \textit{Python} scriptsThe problem of \textbf{different dimensions} is of little importance, solved solely by applying \textbf{resize} algorithms. Images are then all set to be \textit{140x140 pixels}. This is to satisfy two fundamental requirements:
\begin{enumerate}
	\item \textbf{Hardware:} 512x512 pixels images require approximately \textbf{52 GB} of RAM. A similar problem comes out with 256x256 pixels images, whose requirements come at around \textbf{13 GB} of RAM. Then why not 128x128 pixels images? That is due to the second requirement.
	\item \textbf{Inception Model:} Google Inception Networks require for images to have minimum dimensions of 140x140 pixels.
\end{enumerate}
Thus the decision of setting the image dimensions at the standard minimum for both conditions to be satisfied.

\textbf{Encoding}, instead, proves out to be quite a challenge. Images in our dataset show out being encoded with either \textit{12-bit encoding} or \textit{15-bit encoding}. This makes it difficult for the network to read them, as almost white pixels in 12-bit encoding (i.e. a pixel value of 4000 on a maximum of 4096) would turn out to be darkgray in 15-bit encoded images (i.e. a pixel value of 4000 on a maximum of 32768), where in both encodings a lower value of a pixel relates to a darker color \textit{(all images, as already stated, are grayscale)}.

\subsection{Labeling}
Images are labeled corresponding to the anatomical site they represent. In this project two different sets have been chosen to show the potential of the network:
\begin{itemize}
	\item Lungs/NotLungs
	\item Skull/NotSkull
\end{itemize}
They both derivate from the original dataset, being exact copies of it but with a specific labeling according to the anatomic part that has to be extracted. Taking as an example \textit{"Lungs/NotLungs"} in the labeling section two classes will be created, and every image has to be into \textbf{one} class only. This clearly means that there will be no part which is contemporarily both a "Lungs" and a "NotLungs". Classes will be fundamental, as they form the \textit{Ground Truth} on which the whole network will train. It is possible to say that this is the most critical step in the entire process, as labeling errors will result in possibly undetectable accuracy errors in the Neural Networks. Those erros could be misinterpreted into other kinds of errors, thus making the needed process of improvement by trial and error not much as very hard but almost impossibile.
Due to the chosen parts \textit(Lungs/Skull) being just small parts of the human body, it is obivious that there is a huge disparity in numbers between the ones belonging to the section and their counterparts. Randomically 1500 images are taken from both the ones belonging to the target district and the ones not belonging to it. These images will form the \textit{"Prediction Set"}, and are images which the Network will never train on, thus allowing for the results which will follow to be more realistic and untouched by any kind of biases or overfitting problems. 

\section{Pre-Trained Inception}
As stated before, Transfer Learning stands out as a new, working and functional way for Convolutional Neural Networks. Following this alternative we've chosen to use a \Textit{Google Inception V3 Network}, a CNN which is able to achieve excellent performance with contained computational costs.\cite{Szegedy2015a} We've chosen a Pre-Trained model written and developed with \textit{Keras}, whose preventive training was conducted with an ImageNet dataset.\cite{kerasinception} This means that the Network we're going to work on is not a completely virgin network and is instead already partially trained in image recognition, even though the images used to pre-train it have nothing in common with the ones that this project aims to analyze.
%TODO: DESCRIVERE NEL DETTAGLIO LA INCEPTION. Layers, Ottimizzatore, etc...

\section{Training and Testing}
We have conducted numerous tests on this Pre-Trained Inception Network. First of all, the Lungs/NotLungs dataset has been set and prepared. Every session of Training and Testing followed the subsequent order with no exception whatsoever:
\begin{enumerate}
	\textbf{\item Dataset Creation:} First of all the dimension of the dataset which will be fed to the Network is created. Half of the images composing this dataset are taken from the Lungs folder, whilst the other half are taken from the NotLungs folder. The images are copied from their origin, leaving the standard dataset completely untouched. The way the images are selected is by creating a pseudo-random list of names of images in the original folder. The well defined seed of the pseudo-randomic list allows for perfect recreation of every following step, thus making the results always achievable with the same state every time it is needed. The images are then copied into a new directory, called \textit{"Train"}. From there 20\% of images are moved, also by the same randomic means, in a new directory called \textit{"Validation"}. These two directiories hold the new dataset for the whole time the train/test goes on.

	\textbf{\item Train/Test:} The newly created dataset is then fed into the Pre-Trained Inception Neural Network, which trains on all the images belonging to the \textit{"Train"} directory, adjusting his weights accordingly. It does so in \textit{batches} of 64 images: this means that the Network is able to see 64 images at a time and work on them. This number could be changed accordingly to one's will, but it has to be stated that the less it is, the better the Net will be able to train. With a better learning though comes the need of better hardware able to withstand such a workload. Once the CNN has seen all the images it then puts its knowledge to test, by trying to guess on all the images in the \textit{"Validation"} directory. This process, called an \textit{epoch}, is repeated for 10 times. With every epoch the Net trains more and more, learning from its mistakes and adapting to improve its ability to discern the images belonging to the two target classes. At the end of this step it is possible to have a rough idea of how well the net performed: \textit{Accuracy} (the percentage of exact guesses the Net made) and \textit{Loss} (the value of the loss function at the last step) are displayed for both the last training and last validation's epoch. The newly trained model is then saved, thus making the process of Traning/Testing a one-time cost. This is fundamental, as time needed to perform this step could fary from a few minutes for small datsets \textit{(i.e. 5000 images)} to many hours for bigger ones \textit{(i.e. 50000 images)}.

	\textbf{\item Prediction:} In a new script the already trained model is loaded and given the images belonging to the \textit{"Prediction"} directory. The model, now facing images never seen before, has to guess the class of every single one of the 3000 images in the directory. This steps usually takes no more than a few minutes, giving back an \textit{Accuracy} ratio of how well the model performed, also explicating the amount of both False Positive and False Negative results.
\end{enumerate}

This process, exactly as already stated, has been repetead over different sizes of the dataset with different random states, in order to be sure that there was no bias due to the randomly picked images. For the dataset dimensions, the following were chosen, representing the total amount of images to be split into "Train" and "Validation":
\begin{itemize}
	\item 500
	\item 3000
	\item 4000
	\item 5000
	\item 8000
	\item 10000
	\item 20000
	\item 50000
\end{itemize}

Smaller datasets, which we will define as the ones up to the 5000 image treshold, required no more than 1 hour for the whole process whilst bigger datasets required much more, up to the biggest possible dataset of 50000 which required almost 10 hours. Even though the total amount of available images in the dataset is 90000, we've stated 50000 being the biggest possible dataset in our \textit{Lungs/NotLungs} setup due to the fact that Lungs images, which are way less than NotLungs images, come up to a total of 25000 approximately. The 1:1 proportion between images belonging to the selected area and images which do not is decided on a trial and error basis, following the assumption that on a Pre-Trained Network having a way bigger number of images on one side could have broguht to a bias in detection abilities of the CNN itself. The tests have been conducted from one to three times according to the size of the Dataset, where smaller dataset have had more tests. The difference between each test conducted on the same amount of images is the random state with which the images were selected from the starting dataset, thus showing that there is no bias or overfitting imputable to chosen images.

\end{document}