% !TEX root = ../main.tex

\documentclass[../main.tex]{subfiles}
\begin{document}
\label{stateofart}
\thispagestyle{empty}


%Amongst the different types of cancer, Lung Cancer stands out as one of the most common and the deadliest of them all, causing almost 1.6 million deaths every year on a global basis.\cite{Wasserman2015} When detected in later stages (III - IV) it's seldom considered un-operable and most therapies will prove unsuccessful. Only less than 5\% of people diagnosed with lung cancer survive for more than 10 years, but it has been proved that detecting it at earlier stages (I and II) leads to drastic augments in survivability rates,\cite{CancerResearchUK} as it becomes possible to perform possibly effective therapies. Thus, the need of being able to detect lung cancer as soon as possible is extremely important, while also differentiating between benign and malignant nodules in order to provide the best possibile treatment to the patient. Imaging techniques as Computer Tomography (CT) and Positron Emission Tomography (PET) \textit{(Figure ~\ref{ctpet})} are becoming, as of today, a standard as radiologists' aiding tools to perform diagnosis in patients which are suspected (or are already confirmed) to have lung cancer.\cite{Indicators2017} 

%\begin{figure}[!b]
%\centering
%\includegraphics[width=\textwidth]{ctpet}
%\caption{Respectively a CT and a PET scan}
%\source{http://cancergrace.org/wp-content/uploads/2007/01/pet-ct-fusion.jpg}
%\label{ctpet}
%\end{figure}

%Time, as stated beforehand, is a critical factor in lung cancer diagnosis and the use of screening techniques like the ones already mentioned has proved to increase the amount of lung nodules and lung cancers detected at early stages (compared to chest radiography).\cite{AlMohammad2017} Despite the usage of CT and PET, though, time still comes as a scarce resource for radiologists, and the possibility of committing mistakes is always present. A nodule might come by undetected by the human eye if too small or well hid in proximity of the rib cage, whilst a benign nodule might be exchanged for a malignant or viceversa. Therefore it becomes obvious the necessity of Computer Aided Detection (CAD) in providing new faster and more precise ways to perform diagnosis. Where aided by CAD systems, radiologists show less diagnostic errors and false negatives, also being more accurate than a single or even a double radiologist reading.\cite{AlMohammad2017} The concept of CAD is based on the conversion of medical images in data which can be mined, analysed and combined with already acquired knowledge in order to provide support for decision making in medicine. This process, known as \textit{Radiomics}, allows for the extraction of features from images and their subsequent analysis, hence allowing for the developing of models with the potential to improve the accuracy of diagnosis and prognosis.\cite{Gillies2016} 
%\vspace{5mm}
%\section{Radiomics}
%Radiomics allows to detect large datasets, containing huge amounts of data, and extract valuable information from those datas. Informations, in the field of lung cancer detection, refer to features and characteristics of nodules: shape, position, intensity, texture, wavelet, etc. are all features which can be extracted from medical images and analysed in order to obtain support in decision making. \cite{Chen2017} As of today radiologists manually identify, on every single CT slice (from 256 to more than 400 CT slices per patient), a \textit{Region of Interest (ROI)}, containing the supposed nodule. Such ROI will then be analyzed and on it feature extraction will be performed. This process, though, turns out to be extremely tedious and time consuming for radiologist, providing us with an extremely important problem: to find an automatized way to perform lung nodules detection. 
%\vspace{5mm}
%\subsection{Image Processing}
%Image processing is a method which consists in performing operations on an image in order to get an enhanced version of it from which it will be possible to extract valuable informations. Such operations are performed by means of computer algorithms, which allows for such a method to be considered automatised, as it will respect the concepts of: Standardisation, repeatability, reproducibility etc.
%In our specific field of interest image processing is one of the most used methods (if not actually the first) to perform feature extraction. Both PET and CT scans can be used to such a purpose, each one with its own pros and cons:
%\begin{itemize}
%	\item PET: With its high contrast makes it way easier to find nodules and being able to determinate the stage of tumours, but it requires the usage of radioactive materials as the likes of Fluro deoxy-glucose (FDG)
%	\item CT: Cleaner images with low contrast. It is harder to distinguish sane tissue from cancerous tissue, but they're cheaper to acquire and are not potentially harmful to the human body.
%\end{itemize}

%Considering these aspects it is clear how many studies prefer the usage of CT images and work on making the features extracted from those images as valuable as possible. 
%Every work uses its own steps in performing image processing, each and every step personally optimised in order to improve the efficiency of the proposed system, but it is possible to identify a few fundamental steps which are common to most of the works we analysed. \cite{Khan2015,Chaudhary2012, LogeshKumar2016, Amutha2013, Mu, Punithavathy2015}


%\begin{enumerate}
%	\item Pre-Processing: In this step two different kinds of processes are applied in order to prepare the image for following works.
%	\begin{itemize}
%		\item Image Enhancing: Scaling, greyscale level adaptation, contrast, smoothing, etc. There is no standard for a "good" enhanced image, thus the applied filters and their specifics vary from work to work. \textit{(Figure ~\ref{imageprocess})}
		
%\begin{figure}[!b]
%\centering
%\includegraphics{imageprocess}
%\caption{(A) Original Image (B) Example of Gabor Filter. \cite{Chaudhary2012}}
%\label{imageprocess}
%\end{figure}
		
		
%		\item Image Segmentation: This is a critical step, depending on its success the whole process of image processing and feature extraction might be a failure or a success. By applying filters, usually Thresholding and Waterhsed, the image is segmented and only fragments which contain valuable informations are left \textit{(i.e. detected lung nodules)}.
%	\end{itemize}
%	\item Feature Extraction: The input data is now transformed, by means of various algorithms, into analysable data. Even here the extracted features are depending on the subject work, but it is possible to identify a few standard features which are almost always extracted:
%	\begin{itemize}
%		\item Area: Number of pixels which compose the segmented zone.
%		\item Perimeter: Number of the outline of nodule pixel.
%		\item Roundness: Parameter of circularity, with 1 considered as perfect roundness.
%	\end{itemize}
%\end{enumerate}


%Taking into consideration the ones which we have analysed only a few works, like \textit{Chaudary et Al.}\cite{Chaudhary2012}, also perform \textbf{Classification} of the found nodules, identifying benign and malignant ones.
%Most of the proposed methods, though, are not applied to a statistically relevant base, thus only providing us with something which still havs to undergo some critical phases in order to be proved effectively useful. In their work, \textit{Katiyar et Al.} have only applied their algorithm to 5 pre-selected images, and even though the results appear to be promising it is not possible to statistically confirm that it works, or at which extent it does.\cite{Amutha2013} On the other hand \textit{Punithavathy et Al.} apply their algorithm to 888 samples \textit{(source and characteristics of such samples are unknown)}, showing a really promising overall accuracy of 92.67\%.\cite{Punithavathy2015} The aforementioned work, being the only one with a statistical accuracy, will be our reference in considering the overall accuracy of standard image processing methods.
%Thus, even though it is usually really cost-efficient to apply such processes, and the accuracy of the prediction \textit{(where present)} appears to be quite convincing, there are also drawbacks. One of the most problematic ones is the detection of small and juxtaproximal lung nodules, which are at risk of being excluded due to being identified as part of the parenchyma itself during the \textit{Segmentation} phase. Thus comes the need of a more sensible and precise technique for Radiomics, somthing more accurate than the proposed methods, something able to learn from its mistakes. 


\vspace{5mm}
\section{Neural Networks}
\begin{figure}[!b]
  \centering
  \includegraphics[width=0.6\linewidth]{neurons.png}
  \caption{A biological neuron (left) and a perceptron. (right)}
  \source{https://hackernoon.com/how-do-artificial-neural-network-recognize-images-c3699af0f553}
  \label{fig:neuron}
\end{figure}


Neural networks are mathematical models inspired by the human brain cells, the neurons. \textit{(Figure ~\ref{fig:neuron})} \\ 
A biological neuron can be seen as composed by three parts: \textbf{the dendrites}, that receive the input signals from the other neurons, \textbf{the nucleus} which computes the output signal to pass to the next neurons layer and \textbf{the axon} which transmits the nucleus signal forward to other neurons' dendrites. \\
Its simplest mathematical model, \textit{the perceptron}, has multiple inputs $v_{1...n}$ (corresponding to \textbf{the dendrites}), an activation function $\Sigma_{k=1}^n w_k*v_k$ (which computes the various input signals just as \textbf{the nucleus} does) and one output $y_i$ (\textbf{the axon}). To completely mimic the biological neuron, together with its action potential, a \textit{bias} (a real number representing the minimum value required to have an output) is applied as an additional input.


\begin{figure}[!b]
  \centering
  \includegraphics[width=0.8\linewidth]{ann-structure.jpg}
  \caption{A basic artificial neural network structure.}
  \source{ https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks}
  \label{fig:ann-structure}
\end{figure}

Perceptrons, acting as single processing units, are connected in networks to solve complicated problems. There are various types of neural networks (NN) but their basic principles are very similar \cite{Svozil1997}: 
\begin{itemize}
	\item each neuron is able to receive input signals, to process them and send an output signal;
	\item each neuron is connected with at least one neuron in the network; 
	\item each connection is evaluated by a real number, called \textit{weight coefficient}, that reflects the degree of importance of the given connection in the network. (as seen in \textit{Figure ~\ref{fig:ann-structure}})
\end{itemize}
A NN can be seen as an universal approximator, it can realise an arbitrary mapping of one vector space onto another vector space \cite{Svozil1997}: i.e. if the target vector space (whose dimension is M) represents a vector of classes, the NN can realise a classification of the inputs. (with $M=2$ the NN performs a \textit{binary classification}, if $M>2$ the NN performs a \textit{multinomial classification})  \\ \\
A multi-layer forward (MLF) neural network consists of neurons that are organised into layers: the first layer is called \textit{input layer}, the last layer is called \textit{output layer} and the layers between are \textit{hidden layers}. \cite{Svozil1997} (\textit{Figure ~\ref{fig:ann-structure}}) 

\vspace{5mm}
\subsection{Learning process}
What really makes neural networks stand out in classification tasks is their ability to \textit{learn}: to capture an a-priori unknown information in the data is called "learning" (or "training"), and can be done by varying the weights coefficient in such a way that some conditions are fulfilled. \cite{Svozil1997} \\ 
There are two types of training: 
\begin{enumerate}
	\item Supervised training: the NN knows the desired output and adjust the coefficients in such a way that the calculated and desired output are as close as possible;
	\item Unsupervised training: the desired output is unknown, the system is provided a group of facts (\textit{pattern}) and then left to itself to settle down to a stable state.
\end{enumerate}
MLF neural networks, trained with a back-propagation learning algorithm are the most popular neural networks. \cite{Svozil1997} \\
In the back-propagation algorithm the steepest descent minimisation method is  used, but there are some variations \cite{Kuruvilla2014}: \textbf{the gradient descent back-propagation} (in which the weight and bias values are updated due to the gradient descent: an iterative method that computes the directional derivative, \textit{the gradient}, of the loss function to find the values update direction in the N-dimensional space that leads to the minimum mistake), \textbf{the gradient descent with variable learning rate} (gradient descent with adaptive \textit{learning rate}, that can be defined as the magnitude of the update at the $k^{th}$ iteration, in the direction found by computing the gradient), \textbf{the gradient descent with momentum} (gradient descent with \textit{momentum}, which is a coefficient that determines the contribute of the $k^{th}$ values and the learning rate update in finding the $(k+1)^{th}$ values), \textbf{the gradient descent with variable learning rate and momentum}, \textbf{the resilient back-propagation} (a method in which the direction of the update varies according just to the sign of the partial derivatives and not their value, thus eliminating the effect of their magnitude in the direction search), \textbf{conjugate gradient algorithms} (the step size is updated at every iteration, along conjugate directions chosen due to the loss function gradient: this assures the mathematical convergence of the algorithm in N iterations, with N the dimension of the direction searching space), \textbf{quasi-Newton algorithms} (finding the minimum of a function can be seen as finding the zero of the derivative of that function: Newton methods are used in zero-searching and have faster convergence than conjugate gradient methods), \textbf{the one step secant algorithm} (meant to bridge the gap between the conjugate gradient and quasi-Newton's algorithms) and \textbf{the Levenberg-Marquardt}.


\vspace{5mm}
\section{Convolutional Neural Networks (CNNs)}
During the $20^{th}$ century, in order to solve images classification problems, a new type of NN was developed, the \textit{convolutional neural network} (inspired by the recent discoveries in the visual cortex of a cat \cite{Hubel1962}): in these new NNs each unit in a layer (neuron) receives inputs from a set of neurons located in a small neighbourhood in the previous layer.  \\
This means that units in the same layer are organised in planes within which all of them share the same set of weights: the series of convolution outputs of the neurons in a single plane is called \textit{feature map} \cite{LeCun1998} \\
This can be done by computing a convolution between a matrix (whose elements are the desired weights and whose dimension corresponds to the receptive field's), called \textit{filter} or \textit{kernel}, and the input image: the convolution is computed on the whole image by sliding the filter by a certain number of pixels (\textit{stride}) across it after each convolution, activating all the layer's neurons using the same weights. \\ 
The convolution output is easy to interpret as the grade of similarity between a certain feature and the image portion: if the numerical pattern given by the weights of the filter (feature) is matched by the pixel values in the convolution region, the output in the feature map will be the maximum value. \\
Each different plane gives a different feature map as output \textit{(Figure \ref{fig:lenet5})}: an interesting property of convolutional layers is that if the input image is shifted, the feature map will be shifted by the same amount but will be left unchanged otherwise.  \\
The robustness of CNNs to shifts and distortion of the inputs is enforced by adding a so-called \textit{sub-sampling layer}: once a feature has been detected its exact position becomes less important, only its approximate position to other features is relevant. Not only the exact position of the feature is irrelevant, but it is potentially harmful because of its variation in distorted or shifted inputs containing the same features. \\ The solution is to use the sub-sampling layer in order to reduce the resolution of the output, thus reducing the precision with which the position of distinctive features is encoded in the feature maps. \cite{LeCun1998}
It's important to note that convolutional and sub-sampling layers extract features from the input images but do not elaborate them: the classification happens in the last CNNs layers (the so-called fully connected layers) that have the same structure as the hidden layers in MLF neural networks. 
The fully connected layers give the probability of an image to belong to a certain class based on the position and the magnitude of the values in the feature maps they are given as input.

\vspace{5mm}
\subsection{Transfer learning}
Another advantage of using neural networks, is the so-called \textit{transfer learning}: in machine learning, transfer learning refers to application of a process suited for one specific task to a different problem. \cite{Lakhani2018} \\ Since all of the images share same features (such as edges and blobs), a pre-trained convolutional neural network can be used in a very different context with the transfer learning technique: removing only the last fully connected layers, removes the need of filter weights to be learned again, leaving the training process to tune the new classification levels. This reduces the amount of images needed to have performing results and is especially useful iIn medical imaging classification tasks, as it may be difficult to annotate a large dataset to train from scratch. \cite{Lakhani2018} 


\begin{figure}[!b]
  \centering
  \includegraphics[width=\linewidth]{lenet5.png}
  \caption{Architecture of LeNet-5 for hand-written digits recognition \cite{LeCun1998}}
  \label{fig:lenet5}
\end{figure}

\begin{figure}[!b]
  \centering
  \includegraphics[width=\linewidth]{AlexNet}
  \caption{Architecture of AlexNet \cite{Krizhevsky2012}}
  \label{fig:alexnet}
\end{figure}

\vspace{5mm}
\subsection{CNNs architectures}
\begin{description}
\item[LeNet (1994 ca.)] \cite{LeCun1998}. \hfill \\
One of the first convolutional neural network to be designed was the Le-Net 5 \textit{(Figure \ref{fig:lenet5})}, where the number 5 stands for the dimension of the kernel in the convolution layers C1-C3-C5. It consists of seven different layers, including the last fully-connected layers \cite{LeCun1998}. 
\end{description}
After this initial Le-Net model, there hasn't been a large focus on convolutional neural networks until recent years: in 2010, the first ImageNet Large Scale Visual Recognition Challenge (ILSVRC) was organised \cite{Russakovsky} and has quickly became a benchmark for the state of the art of CNNs image classification. It consists of two components: the public common dataset and the annual competition to vote the most successful entry every year. The competition tasks are: image classification, single-object localisation and object detection. \cite{Russakovsky}

\begin{description}
\item[AlexNet (2012)] \cite{Krizhevsky2012} \hfill \\
The AlexNet won the ILSVRC competition in 2012; this new architecture consists of 5 convolutional levels and 3 fully connected layers \textit{(Figure \ref{fig:alexnet})}. The most distinctive features are: \textbf{ReLU (REctified Linear Unit)} as neurons' activation function ($f(x)=max\{0,x\}$) instead of the usual $tanh(x)$ to improve the training speed, \textbf{multiple GPUs}(working on two parallel pipelines and communicating only on certain levels, as seen in \textit{figure \ref{fig:alexnet}}), \textbf{local response normalisation} and \textbf{overlapping pooling}, to reduce the overfitting \cite{Krizhevsky2012} (\textit{overfitting} is a term to express a poor generalisation capacity of the network)
\item[NIN: Network In Network (2013)] \cite{Lin2013} \hfill \\ 
To overcome the poorly abstraction capacity of traditional CNNs, a research team from Singapore had the idea to use a more potent non-linear function approximator such as a multilayer perceptron \textit{(Figure \ref{fig:nin})}: instead of using linear filters followed by a non-linear activation function to scan the input, they proposed a micro neural networks with more complex structures to abstract the data within the receptive field. \cite{Lin2013}. Each layer still outputs a feature map, but since the multilayer perceptron is used, there is no need of fully connected layers, the output is the spatial average of the feature maps instead (this prevents overfitting and is also more interpretable) \cite{Lin2013}
\item[VGG-net (2014)] \cite{Simonyan2015} \hfill \\
The VGG-net (named after the team that came up with this model, the Oxford's Visual Geometry Group) placed second at the 2014 ILSVRC edition. This model is inspired by the original LeNet 5 \cite{LeCun1998} but instead of having each convolutional layer followed by a sub-sampling layer, it stacks multiple convolutional layer together with a small 3X3 kernel\textit{(Figure \ref{fig:vggnet})}: this reduces the computational cost, but guarantees the same effective receptive field as using a bigger 7X7 kernel.\cite{Simonyan2015}
The VGG-net has 5 max-pooling layers and 3 fully-connected layers; unlike the AlexNet, it doesn't have a local response normalisation (to avoid memory consumption) and doesn't have a sub-sampling overlapping. 
\item[GoogLeNet/Inception (2014)] \cite{Szegedy2015} \hfill \\ 
The GoogLeNet beat the VGG-net arriving first at the 2014 ILSVRC competition: the idea behind this model is that, in order to be more accurate and improve the state of the art performance, stacking layers and create bigger models is not a good idea: first, the computational cost rise with increased layers and second, having more coefficients leads to a higher overfitting probability. \\ 
To solve both issues, the solution is to move from fully connected to sparsely connected architectures: Inception architecture (another name for this network) is based on finding how an optimal local sparse structure in a convolutional vision network can be approximated and covered by readily available dense components, and repeat it spatially.\cite{Szegedy2015}
This mean that the proposed architecture is composed by the Inception modules \textit{(Figure \ref{fig:blocks} (a))} stacked on top of each other: the result is a combination of all those layers with their output filter banks concatenated into a single output vector forming the input of the next stage

\item[ResNet (2015)] \cite{Wu2017} \hfill \\
2015 ILSVRC winner, the idea behind the ResNet is to use a VGG-net structure passing every two convolutional layer an identity shortcut (a residual) \textit{(Figure \ref{fig:blocks} (b))}: let $\mathcal {H}(X)$ be the underlying mapping function to be learned, and let $\mathcal{F}(X)=\mathcal {H}(X) + X $ be another mapping that the stacked nonlinear fit. The original mapping is recast into $\mathcal{F}(X)+X$ this residual mapping is easier to optimise than the original, unreferenced mapping. \cite{Wu2017}
\end{description}

\begin{figure}[!hb]
  \centering
  \includegraphics[width=\linewidth]{Nin}
  \caption{NIN (Network In Network) \cite{Lin2013}}
  \label{fig:nin}
\end{figure}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=\linewidth]{VGG.png}
  \caption{Architecture of VGG-net}
  \source{https://www.researchgate.net/figure/Architecture-of-the-VGG-network\_\ fig1\_\ 303993422}
  \label{fig:vggnet}
\end{figure}



\begin{figure}[!ht]
   \centering
    \begin{subfigure}{0.4\linewidth}
        \includegraphics[width=\linewidth]{inception_module}
        \caption{An Inception module} \cite{Szegedy2015}
    \end{subfigure}
    \begin{subfigure}{0.4\linewidth}
        \includegraphics[width=\linewidth]{residual_block}
        \caption{A residual block} \cite{Wu2017}
    \end{subfigure}
   
  \caption{The new deep neural networks modules}
  \label{fig:blocks}
\end{figure}


%\item[ResNet (2015)] \cite{Wu2017} \hfill \\
%2015 ILSVRC winner, the idea behind the ResNet is to use a VGG-net structure passing every two convolutional layer an identity shortcut (a residual) \textit{(Figure \ref{fig:blocks} (b))}: let $\mathcal {H}(X)$ be the underlying mapping function to be learned, and let $\mathcal{F}(X)=\mathcal {H}(X) + X $ be another mapping that the stacked nonlinear fit. The original mapping is recast into $\mathcal{F}(X)+X$ this residual mapping is easier to optimise than the original, unreferenced mapping. \cite{Wu2017}
%\end{description}
%Being an identity shortcut, this procedure doesn't add any computational cost to the model and can easily be implemented and optimised as the other CNNs. \cite{Wu2017}

%\end{description}

%\begin{figure}[!hb]
%  \centering
%  \includegraphics[width=\linewidth]{Nin}
%  \caption{NIN (Network In Network) \cite{Lin2013}}
%  \label{fig:nin}
%\end{figure}

%\begin{figure}[!hb]
%  \centering
%  \includegraphics[width=\linewidth]{VGG.png}
%  \caption{Architecture of VGG-net}
%  \source{https://www.researchgate.net/figure/Architecture-of-the-VGG-network\_\ fig1\_\ 303993422}
%  \label{fig:vggnet}
%\end{figure}

%\begin{figure}[!hb]
%   \centering
%    \begin{subfigure}{0.4\linewidth}
%        \includegraphics[width=\linewidth]{inception_module}
%        \caption{An Inception module} \cite{Szegedy2015}
%   \end{subfigure}
%    \begin{subfigure}{0.4\linewidth}
%        \includegraphics[width=\linewidth]{residual_block}
%        \caption{A residual block} \cite{Wu2017}
%    \end{subfigure}
   
%  \caption{The new deep neural networks modules}
%  \label{fig:blocks}
%\end{figure}

%\end{description}

\clearpage
\newpage
\section{Automatic body-part recogntion}
With a widespread use of digital imaging data in hospitals, the size of medical image repositories is increasing rapidly: this causes difficulty in managing and querying these large databases leading to the need of automatic medical image retrieval systems. \cite{Qayyum2017} \\
As stated in the introduction, an auto-bodypart recognition algorithm benefits radiological workflow in different aspects; a bodypart-based query in the hospital's picture archiving and communication system (PACS) could be based on the text information contained in the DICOM header, but, since the bodypart information in it is not very reliable, an automatic bodypart recognition will enable content-based image retrieval and improve the retrieval precision. \cite{Yan2016} \\ \\
\textbf{Content based image retrieval} (CBIR) is a computer vision technique that gives a way for searching relevant images in large databases. This search is based on the image features like color, texture and shape or any other features being derived from the image itself. The performance of a CBIR system mainly depends on these selected features \cite{Qayyum2017}: since convolutional neural networks don't need any fixed features but learn them from the images instead, they can be a better approach for bodypart CBIR.\\
In fact, traditional methods normally use hand-crafted image classification features; lately, deep learning approaches have been adopted with promising results where Convolutional neural networks (CNN) are employed to learn deep image features. \cite{Yan2018}
Furthermore, the rapid adoption of deep learning may be attributed to the availability of machine learning frameworks and libraries to simplify their use. \cite{Lakhani2018} \\
In this work we demonstrate how, using accessible tools such as Keras (a public CNNs Python high-level library) and a pre-trained Inception network \cite{Lakhani2018}, we were able to build an automatic bodypart recognition algorithm that using manually labelled .png files obtained from both public and private anonymised CT scans in DICOM format, reaches an accuracy that can be compared to the state of the art's. \cite{Qayyum2017} \cite{Yan2016} \cite{Yan2018}


\end{document}