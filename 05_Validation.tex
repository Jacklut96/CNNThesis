\documentclass[../main.tex]{subfiles}
\begin{document}
%\chapter{Validation, evaluation,  potential}
\label{capitolo4}
\thispagestyle{empty}

\begin{figure}[b]
\caption{Lungs/NotLungs classification error (1)}
\centering
\includegraphics{lungerror}
\label{lungerror}
\end{figure}

\begin{figure}[b]
\caption{Lungs/NotLungs classification error (2)}
\centering
\includegraphics{lungerror2}
\label{lungerror2}
\end{figure}

\begin{figure}[b]
\caption{Near shoulder blades lungs section}
\centering
\includegraphics{lungblade}
\label{lungblade}
\end{figure}

\begin{figure}[b]
\caption{(A)Skull Section (B)Lungs Section (C)Abdomen Section}
\centering
\includegraphics{ctcomparison}
\label{ctcomparison}
\end{figure}

In this chapter we will observe the results obtained from the aformentioned tests \ref{capitolo3}. Such results will then be anayzed and discussed, contestualizing them into the project concept and considering their value.

\section{Test Setup}
All the following tests have been executed on a machine with \%TODO: INSERIRE SPECIFICHE DWARF.
The tests have been performed on two different datasets, prepared as defined in \ref{capitolo3}: this was basically due to time costraints, and even though only Lungs/NotLungs and Skull/NotSkull datasets have been used, such is not to be considered a limitation of the architecture. As will be shown later, the obtained results will be good enough to almost assure that the model is capable of detecting any kind of anatomic site with high accuracy results.
With a source code written entirely in Python and datasets with lots of images, long computational times were to be expected. On the aformentioned architecture training/testing times spaced within a range of 30 minutes to 10 hours circa, being extremely dependant on used dataset dimension. Such a higly time-costing process, though, is one-time only, because once trained the now ready model is saved and can then be loaded and extensively used without any kinds of limitation. When the already trained model is then used to perform classification on never seen images, a process which will be referred to as \textit{Predicting}, execution time is estimated at around 3 minutes for a 3000 images dataset. Considering how full CT exams rarely have more than 500 images, it is possible to conclude that the expected time for Prediction is even lower, even though most of its computational cost comes from loading the trained model.

\section{Results}
The results obtained will be presented in two different tables: one for the \textit{Lungs/NotLungs} dataset and the other for the \textit{Skull/NotSkull} one. Every table will though have the same attributes, whose description is now given:

\begin{itemize}
	\item \textit{Train Accuracy:} Percentage of correctly \textit{"guessed"} (label assigned by the CNN and ground truth match) images in the last epoch (10th) of training.
	\item \textit{Train Loss:} Value of the loss function in the last epoch (10th) of training.
	\item \textit{Validation Accuracy:} Percentage of correctly guessed images in the last validation step (full trained network).
	\item \textit{Validation Loss:} Value of the loss function in the last validation step (full trained network).
	\item \textit{Prediction Accuracy:} Percentage of correctly guessed images in the prediction dataset.
\end{itemize}

The difference between Validation Accuracy and Prediction Accuracy is of utmost importance: Validation images are seen by the CNN at the end of every step of traning, working as a "training field" on which the Net can check its performance. It is a fundamental step in machine learning, but its results are more than often not valuable as benchmarks. Thus the need of a \textbf{Prediction Dataset}: All the images it contains are never seen by the Network during its training, so the results provided by this step are functional to be defined as benchmarks for the CNN's efficiency. Furthermore, as stated before, for some dataset sizes more than one test has been executed: In such cases the shown value will be the exact \textbf{mean} of all the achieved results.

\subsection{Lungs/NotLungs}
The following table shows the results coming from the test performed on the Lungs/NotLungs Dataset. The whole dataset is composed of 25000 images labeled as \textit{"Lungs"} and 75000 labeled as \textit{"NotLungs"}. As already stated, such a difference in size between the "target" and the "general" sections is to be expected as Lungs, for example, compose only a part of the human body. The process which describes how the Prediction Accuracy, which can be considered the "final" result, is achieved is completely explained in \ref{capitolo3}.
\begin{table}
\begin{tabular}{cccccc}
Dataset Size & Train Accuracy (\%) & Train Loss & Validation Accuracy (\%) & Validation Loss & \textbf{Prediction Accuracy (\%)} \\
3000         & 98,92          & 0,0325     & 93,06               & 0,2410          & \textbf{96,91}               \\
4000         & 99,69          & 0,0090     & 96,09               & 0,1756          & \textbf{97,13}               \\
5000         & 99,69          & 0,0105     & 97,36               & 0,1316          & \textbf{97,43}               \\
8000         & 99,72          & 0,0087     & 97,19               & 0,1356          & \textbf{97,60}               \\
10000        & 99,56          & 0,0131     & 97,89               & 0,0962          & \textbf{97,89}               \\
20000        & 99,55          & 0,0117     & 97,96               & 0,0614          & \textbf{97,7}                \\
50000        & 99,59          & 0,0115     & 98,65               & 0,0447          & \textbf{98.6}               
\end{tabular}
\caption{\textit{Lungs/Not Lungs} Results}
\label{table:LNLres}
\end{table}

As it can be seen in the table above \textbf{Prediction Accuracy}, which is the \textit{golden standard} to determinate a Net's efficiency, proportionally rises with the \textbf{Dataset Size}. Even though Prediction Accuracy is always pretty high, never falling below 96\%, there is a considerable gap between the 3000 images dataset and the 50000 one. Training the 3000 dataset took less than 1 hour, whilst training the 50000 one took around 10 hours. The difference in time consumption between these is kinda huge, but taking into consideration the fact that is a one-time only cost to substain and that in the medical field a 2\% difference is extremely important, it is possible to define how the 50000 images dataset is the \textbf{best} possible choice between the searched ones. Also because no difference in time comes out whilst using the already trained model to only execute predictions.

\subsection{Skull/NotSkull}
The results for this tests come out with the same structure as the ones before. The complete dataset is composed of 11000 images labeled as \textit{"Skull"} and 89000 images labeled as \textit{"NotSkull"}. The difference between these two is even bigger than the one observed in the \textit{Lungs/NotLungs} section, due to the disparity in size between skull and lungs, the latter being clearly more dimensionally relevant. The methods used to obtain these are the same as before, described in \ref{capitolo3}.

\begin{table}[]
\begin{tabular}{cccccc}
Dataset Size & Train Accuracy (\%) & Train Loss & Validation Accuracy (\%) & Validation Loss & \textbf{Prediction Accuracy (\%)} \\
3000         & 1                   & 2,42E-04   & 99,65                    & 0,0129          & \textbf{99,53}                    \\
4000         & 99,97               & 0,0059     & 99,87                    & 0,0021          & \textbf{99,69}                    \\
5000         & 1                   & 8,28E-05   & 99,79                    & 0,0137          & \textbf{99,65}                    \\
8000         & 1                   & 3,27E-05   & 99,88                    & 0,0068          & \textbf{99,69}                    \\
10000        & 1                   & 8,00E-05   & 99,70                    & 0,0022          & \textbf{99,69}                    \\
20000        & 1                   & 0,0016     & 99,67                    & 0,0111          & \textbf{99,90}                   
\end{tabular}
\caption{\textit{Skull/Not Skull} Results}
\label{table:SNSres}
\end{table}

Exactly as expected, \textit{Skull/NotSkull} follows the same logic as \textit{Lungs/NotLungs}, with \textbf{Prediction Accuracy} proportionally rising with the number of images in the dataset. The overall higer accuracy of this test compared with the other one is to be expected and is easily explained: Skulls, in CT scans, are extremely different from other body parts as it's easily seen in \ref{ctcomparison}. Lungs instead, being in the toracic cage, have a bigger risk of being misinterpreted as other anatomic sites. It also has to be noticed how the expected increase was basically not present in the 4000 to 10000 datasets, whilst appearing with a really high (proportionally) augmentation in the 20000 images dataset. This confirms what was also stated before, so that the one-time huge cost to train a CNN with as much data as possible is repayed by a better accuracy. Even in this different section, indeed, we could confirm the biggest dataset (20000 images) being the \textbf{best} one. 

\section{Errors and Solution}

Even though the best \textbf{Prediction Accuracies} come out as \textbf{98.6\%} for \textit{Lungs/NotLungs} and an astonishing \textbf{99.9\%} for \textit{Skull/NotSkull}; they're still below the perfect textit{100\%} mark which would ideally be the target. A Python script has then been used in order to identify which were the images not being correctly identified and to understand the nature of the error. The possibility of the model \textbf{overfitting}, which means it's adapting too well to training data and failing to predict further never-seen data\cite{overfitting} is always present and must be avoided at any cost.
The figures \ref{lungerror} and \ref{lungerror2} show two of the errors which were most common in the \textit{"lungs/NotLungs"} classification. The aforementioned images were labeled \textit{Lungs} and predicted \textit{NotLungs}. As it can be seen both images, which represent the shoulder blades and trachea section, have minimum if no trace at all of lungs like, for example, figure \ref{lungblade}. All of the ecountered errors were very similar to the now presented situation, allowing us to come up with a basilar but functional deduction: The mistakes which were made during the prediction phase are not to be assigned to any kind of overfitting, due to them being caused by errors in \textbf{labeling} during earlier stages of dataset preparation. This is to be attributed to the inexperience of people assigned to label those images: Labels taken too wide \textit{(i.e. shoulder blades considered as lungs)} or too short \textit{(i.e. lungs'sections closer to the abdomen labeled as not lungs)}. With images correctly labeled by professional doctors such a problem would not have been present and it would have then been possibile to try and achieve perfect performance.

\section{Confrontation}
TODO