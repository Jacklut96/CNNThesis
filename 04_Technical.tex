\documentclass[../main.tex]{subfiles}
\begin{document}
\label{capitolo3}
\thispagestyle{empty}

This chapter focuses on a detailed description of how the proposed solution is executed. Every step covered will be clearly explained, going from the dataset creation and preparation, to the precise specifics of the used network, to how dataset and network interact. Focus will also be given onto how the tests are executed, allowing for maximum repeatability as long as datasets are available.

The first step in designing a CNN is defining the system on which it will be built on. Secondly a Dataset has to be created, and only then it is possibile to perform training, testing and evaluations and conclude whether the built architecture is functional or not. 

\section{Language and Application Programming Interface \textit{(API)}}
\subsection{Python}
\textbf{Python} is an \textit{Object-Oriented} programming language: Currently one of the most popular, the extreme value of \textit{Python} comes both from its extremely readable and elegant syntax and from its constantly developing community. The \textit{Python Software Fundation License}\cite{pythonlicense}	allows to everyone complete free developing of \textit{Application Programming Interfaces (APIs)}: sets of definitions, functions and tools to be used in software development, like the ones which will be later introduced, thus providing with lots of already prepared content to be used in order to save time and resources for more important tasks. Furthermore Python is easy to write and easy to read, making it an excellent choice in the end. The most prominent downside to be considered when choosing Python as a programming language is that it is an \textit{Interpreted Language}: Interpreted languages like Python, in contrast to \textit{Compiled Languages} like C++, have to be converted to machine instructions during the software execution, commonly referred to as \textit{"runtime"}. This makes interpreted languages slower during execution and not perfectly adaptable to scripts which have to be rapidly executed. In the context of this project runtime speed is not to be considered relevant, hence no problem is presented in the choiche of Python.
\vspace{5mm}
\subsection{Tensorflow and Keras}
\textbf{Tensorflow} is a free open source \textit{library} (an example of API composed of documentations, already written code, defined function etc. which can be used anywhere in the software) written in \textbf{Python} and developed by Google Brain, which is excellent for machine learning purposes\cite{Tensorflow2017}: it contains an extremely huge amount of functions and instructions which can be used in order to build a Convolutional Neural Network. Tensorflow's performance can be compared, if not assumed completely better, than other available machine learning libraries (like \textit{Caffe} and \textit{Theano}); its documentation is also easily understandable and well written, plus it gets constantly updated and can be basically defined as the current State of the Art in Machine Learning libraries. Due to the aforementioned reasons, \textit{Tensorflow 1.10} has been defined as the backend to use in the construction of the proposed CNN model. \textbf{Keras}, furthermore, is an \textit{High-Level API}: It allows to read and write code in easier and more user-friendly ways. Code written by means of High-Level APIs like Keras is more abstract, giving much more focus on parts relative to the code itself, automatically including more basic commands in the provided functions, methods etc. Keras also enhances the speed at which the whole network works: Without using it the whole network would require much more code, be less understandable and also extremely slower. During a preliminary test with a pre-prepared dataset and two identical architectures \textit(CNNs with the same structure with the same workflow and characteristics) the usage of Keras has shown to improve timing by a huge margin \textit{(4 hours whilst only using functions provided by Tensorflow against 20 mins ca. with the Keras API)}. 

\section{Dataset Preparation}
The dataset is the collection of data which is used to train, test and validate the CNN. The dataset used here is built by merging both private and public data, where the public data has been obtained from different sources all coming from \textit{The Cancer Imaging Archive (TCIA)}, for a total of more than \textbf{90.000} different images. The combination of different sources to build our dataset is of extreme importance, as it helps the generalisation of the whole system. Different images from different sources, taken from different machines with different settings, allow the CNN to have a better understanding of the possible natural variations of CT Images caused by different instrumentation used. All the data, as soon as taken, is deprived of its \textit{DICOM header} or any kind of \textit{metadata}, if existent, and shuffled with all other data in order to provide total and complete anonymisation. This simple but effective process makes it almost impossible, in any way, to obtain any kind of personal or sensible information from the data used in this project.
All the images composing the dataset are originally greyscale DICOM files \textit{(.dcm)}, but all of them have different dimensions and encoding, bringing forth the need of preprocessing.
\vspace{5mm}
\subsection{Image Preprocessing}
The following processes are all executed by means of \textit{Python} scripts: the problem of \textbf{different dimensions} is of little relevance, and it is solved by simply applying a \textbf{resize} algorithm, a process which sets image dimension to a fixed \textit{140x140 pixels}. Images are then converted into .png format in order to satisfy three fundamental requirements:
\begin{enumerate}
	\item \textbf{Hardware:} 512x512 pixels images require an approximate usage of \textbf{52 GB} of RAM. A similar problem comes out with 256x256 pixels images, whose requirements come at around \textbf{13 GB} of RAM. 128x128 pixels images would instead require the usage of \textbf{3 GB} of RAM, which would be a perfect agreement between kept informations and dimensions, but that ain't possible due to the following requirement.
	\item \textbf{Inception Model:} Google InceptionV3 Networks require for images to have minimum dimensions of 140x140 pixels.
	\item \textbf{Keras:} The \textit{flow\_\\from\_\\directory} method written in Keras which is used to feed images directly into the network does not accept .dcm files, thus the need of converting them into an acceptable format. Between the available ones \textit{.png} format has been chosen, as it shows to have lossless compression, which allows for an excellent compromise between quality and dimension of the compressed image.
\end{enumerate}
In order to satisfy the aforementioned conditions it has been decided to se image dimensions at the standard minimum for both the architecture and hardware limitations, furthermore converting them to .png files.

\textbf{Encoding}, instead, proves out to be quite a challenge: the images belonging to the dataset showed out extreme differences in encoding, which could either be \textit{12-bit encoding} or \textit{15-bit encoding}. This made it difficult for the network to read them, as almost white pixels in 12-bit encoding (i.e. a pixel value of 4000 on a maximum of 4096) would turn out to be dark-gray in 15-bit encoded images (i.e. a pixel value of 4000 on a maximum of 32768), where in both encodings a lower value of a pixel relates to it having a darker color \textit{(all images, as already stated, are grayscale)}.
\vspace{5mm}
\subsection{Labeling}
Images are labeled corresponding to the anatomical site they represent. In this project two different sets have been chosen to show the potential of the network:
\begin{itemize}
	\item Lungs/NotLungs
	\item Skull/NotSkull
\end{itemize}

They both derivate from the original dataset, being composed by the dataset images with a specific labelling according to the anatomic part that has to be extracted. Taking as an example \textit{"Lungs/NotLungs"}, in the labelling section two classes are created, and every image will fit into \textbf{one} class only. This clearly means that there will be no part which is assigned to both "Lungs" and "NotLungs". Classes are fundamental, as they form the \textit{Ground Truth} on which the whole network will train and learn. It is possible to say that this is the \textbf{most critical step} in the entire process, as labelling errors will result in possibly undetectable accuracy errors in the model. Those errors could be misinterpreted into other kinds of errors, thus making the training not much as very hard but almost impossibile.
Due to the chosen parts \textit(Lungs/Skull) being just small parts of the human body, it is obvious that there is a huge numerical disparity between the images belonging to the chosen section and their counterparts.
\vspace{5mm}
\subsection{Prediction Set}
Once all the data goes through the already described process a \textit{"Prediction Set"} is formed by randomically picking approximately 1000/1500 images per class out of the 90.000 total ones. The precise dimension of this specific Set varies depending on how numerically present is the target site in the human body: For lungs, which are relatively big, 1500 images will be good; on the other side for the skull around 1000 images will suffice, as it is way smaller. The characteristic of this Prediction Set is that the CNN will never train neither validate on the images belonging to it. This Dataset has the single and extremely important task of being the final proving ground for the architecture, hence allowing for the results obtained by it to be as realistic as possible: those results will later be the \textbf{benchmark} on which a network's usefulness is judged on.\cite{Karpathy2018}

\section{Pre-Trained Inception}
As stated before, the choice is to use a pre-trained \textit{Google Inception V3 Network} (a CNN which is able to achieve excellent performance with contained computational costs.\cite{Szegedy2015a}) written and developed with \textit{Keras}, whose preventive training was conducted with the ImageNet dataset.\cite{kerasinception} This means that the network on which work is to be executed is already partially trained in image recognition, even though the images used to pre-train it have nothing in common with the ones that this project aims to analyse. \textit{(ImageNET images consist of random images of cats, dogs, ships and other common things which can be found everywhere.)}
The Inception architecture is composed of two fundamental blocks: The \textit{Feature Extraction Module} and the \textit{Feature Classification Module}. Both of these modules contain smaller steps of \textit{Convolution, Pooling, ReLU, Dropout, FullyConnected} etc. As the Model itself is already pre-trained the first module is left untouched, whilst efforts are put in the second module to adapt it to needs of the work it has to execute.\ref{fig:inceptionv3} In the Feature Classification Module numerous things are set up according to what is needed: the classification is performed by Fully Connected Layers activated by the \textit{ReLU (Rectified Linear Unit)} function and a \textit{Dropout Layer} with a 0.5 value is used to avoid overfitting, whilst the final classification is performed by a Layer with a \textit{Sigmoid} Activation. The result given by the network is a number between 0 and 1 for every image: The closer it is to 0 the more the Network is certain about that image belonging to the target site \textit{(i.e. Lungs/Skull)}, the closer it is to 1 the more the Network is certain about said image belonging to the counterpart \textit{(i.e. Not Lungs/ Not Skull)}. Any image with a score equal or below 0.5 is classified as an image belonging to the target section and viceversa. The equality is given to the target section as, in medical analysis, it is considered definitely better to have false positives rather than false negatives; meaning that it is better for an image not in the target section to be considered rather than an image belonging to it being completely ignored and passed over. 
\begin{figure}[h!bt]
\centering
\includegraphics[width=0.8\linewidth]{inceptionv3.jpg}
\caption{Example of GoogleInceptionV3 Architecture}
\label{fig:inceptionv3}
\end{figure}

\section{Training and Testing}
Different and numerous tests are then performed on this Pre-Trained Inception Network. Every session of Training and Testing follows the subsequent order with no exception whatsoever:
\begin{enumerate}
	\item \textbf{MiniDataset Creation:} First of all the dimension of the MiniDataset, this is how the datasets which are directly used by the network will be reffered to from now on, is decided (the list of possible dimensions is later shown in detail). Images are then copied from their origin, leaving the standard dataset completely untouched so that following tests are not influenced by earlier ones. The way images are selected is by creating a pseudo-randomic list of the ones present in the original folder with a perfect 50:50 split between images classified as target site \textit{(i.e. lungs/skull)} and the other ones \textit{(i.e. notlungs/notskull)}. For example, a 3000 \textit{"Lungs/NotLungs"} images dataset means that 1500 images are randomly chosen between all the available ones in the \textit{"Lungs"} section; moreover the same happens with the \textit{"NotLungs"} ones. The well defined seed of the pseudo-randomic list allows for both great randomicity and perfect recreation of every following step, thus making the results always achievable with the same state every time it is needed. The chosen images are hence copied into a MiniDataset called \textit{"Train Set"}. From there 20\% of images are moved, by the same randomic means as before, in a second MiniDataset called \textit{"Validation Set"}. These two sets work together and hold the newly created MiniDataset for the whole time the train/test goes on.

	\item \textbf{Train/Test:} The created MiniDataset, composed by both \textit{Train} and \textit{Validation}, is fed into the Pre-Trained Inception Neural Network: The Net \textbf{trains} on all the images belonging to the \textit{"Train Set"}, which means that by looking at the images and their label (which represents the ground truth) it adjusts his weights accordingly, learning how to differentiate between the ones belonging to the target section and their counterpart. It does so in \textit{batches} of 64 images: this means that the Network is able to see 64 images at a time and work on them. This number could be changed accordingly to one's will as it is not something related to the CNN's architecture, but a few things have to be stated relating to this number: fewer images per batch means that the Net trains on a small amount of images at a time, better training on them. With more accurate learning, though, comes the need of stronger hardware able to withstand such a workload. 64 images, the aforementioned number, comes out as a perfectly balanced pick between accuracy and hardware requirements. Once the CNN trains on all the images it then puts its knowledge to test, trying to guess on all the images belonging to the \textit{"Validation Set"} and confronting the performed guess with the available ground truth. This whole process of training and testing is called \textit{"epoch"}, and it is repeated for 10 times. With every subsequent epoch the Net trains more and more, learning from its mistakes and adapting to improve its ability to discern the images belonging to the two target classes. At the end of the whole process, once all of the epochs have been executed, it is possible to have a rough idea of how well the net performed. Two different parameters are used to evaluate so: \textit{Accuracy} (the percentage of exact guesses the Net made) and \textit{Loss} (the value of the loss function at the last step) are displayed for both the last training and last validation's epoch. The newly trained model is then saved, thus making the process of Traning/Testing a one-time cost. This is fundamental, as time needed to perform this step could vary from less than half an hour for small datasets \textit{(i.e. 5.000 images)} to many hours for bigger ones \textit{(i.e. 50.000 images)}.

	\item \textbf{Prediction:} The already trained model is loaded and given the images belonging to the \textit{"Prediction Set"}. The model, facing images never seen before and whose ground truth it does not know, has to guess the class of every single image in the aforementioned set. This steps, which usually takes no more than a few minutes, gives back an \textit{Accuracy} ratio of how well the model performed, also explicating the amount of both False Positive and False Negative results. This result, which is not influenced by the images in the former step, is a perfect simulation of how the trained CNN would perform in a realistic at-use situation. Taking such into consideration, the results obtained from this step are to be considered the \textbf{benchmark} of how the CNN performs.
\end{enumerate}

This whole process, exactly as already stated, is repeated over different sizes of the dataset with different random states, in order to be sure that the randomic image picking does not generate any kind of unwanted error. The MiniDatasets' different dimensions are shown in \ref{tab:testsperdata}, representing the total amount of images to be split into \textit{"Train Set"} and \textit{"Validation Set"}. For every different size it has then been decided a number of test to be executed: The difference between each test conducted on the same amount of images is the \textbf{random seed} with which the images were selected from the starting dataset. 

\begin{table}
\centering

\caption{Tests Per Dataset}
\label{tab:testsperdata}

\begin{tabular}{|c|c|}
\hline
Dataset Size & Number of Tests            \\
\hline
3000         & 3                          \\
4000         & 3                          \\
5000         & 3                          \\
8000         & 2                          \\
10000        & 2                          \\
20000        & 1                          \\
50000        & 1 (\textit{Lungs/NotLungs} only)    \\
\hline
\end{tabular}
\end{table}

Smaller datasets, which are define as the ones up to the 5.000 image threshold, require no more than 1 hour for the whole process to be executed, whilst bigger datasets require much more time, up to the biggest possible dataset of 50.000, which requires almost 10 hours. Even though the total amount of available images in the dataset is 90.000, 50.000 is stated the biggest possible dataset in the \textit{Lungs/NotLungs} setup due to the fact that Lungs images, which are way less than NotLungs images, come up to a total of 25.000 approximately. The same test could not be applied to the \textit{Skull/NotSkull} dataset for the same aforementioned reason (it was not possible to obtain more than 10.000 skull labeled images from the available dataset, thus the biggest available dataset comes out as the \textit{20.000} threshold). The 1:1 proportion between images belonging to the selected area and images which do not is decided on a trial and error basis, following the assumption that, on a Pre-Trained Network, having a way bigger amount of images on one side could have brought to overfitting in detection abilities of the CNN itself. The tests are conducted from one to three times according to the size of the MiniDataset, where smaller ones have the possibility of more tests being perform rather than bigger ones, due to time availability limitations.

\end{document}